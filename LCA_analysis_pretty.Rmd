---
title: "Latent Class Analysis"
output: word_document
bibliography: bibliography.bib
---

```{r setup & library, include=F, message=F, warning=F}
#Do not show code chunks in the knitted document
knitr::opts_chunk$set(echo=F, warning=F, message=F)

#For reproducible results
set.seed(42)

#Turn off scientific notification
options(scipen=999)

#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM", "rio", "ggpubr", "flextable")

#Load function to check whether required packages are installed & load required packages
source("./functions/check_required_packages.R")
check_required_packages(packages)
lapply(packages, require, character.only=T)

#Clean up
rm(packages)

#######ATTENTION!!!
#At this point we load a pre-saved .RData file in which the environment is saved. That makes the code less reproducible, but removing the eval=F specifications in all code blocks will make that possible 
load(".RData")
```
<!--We begin by loading the required data set ("dataE_2112.sav") into R. The selected variables are: gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr

The performed analyses are exploratory.-->
```{r data, include=F}
#We use gender instead of sex. A 2 represents a male. We want to only grab males. 
data_males <- haven::read_spss("./data/data raw/dataE_2112.sav") %>% dplyr::select(gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% filter(gender == 2) %>% dplyr::select(-gender)

#The same for females
data_females <- haven::read_spss("./data/data raw/dataE_2112.sav") %>% dplyr::select(gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% filter(gender == 1) %>% dplyr::select(-gender)

#We refrain from renaming the variables to make interpretation across analyses easier. 
```

After the initial approach (i.e. investigating both genders at once with an LPA and OPTICS) did not result in any interpretable solutions, the data set was split into participants who identified as male and those who identified as female, which were then analysed separately. Because of the highly skewed distributions of some of the variables (see Figures X and Y), we decided to split the scales into quantiles (see Figures X and Y), and, when sufficient data to split the data into quantiles was not available, to dichotomise them. This was the case for sexual interest in children with both male and female participants (where 97% of the participants completely denied sexual interest in children, i.e., selected 0 on all three items), as well as compulsive sexual behaviour and problematic porn consumption with the female participants. Due to the different answering behaviour in the male and the female sub-samples, the quantiles necessarily vary.

__Figure X__

*Scale Distributions With Quantiles for Male Participants*
```{r violin plots, fig.width=12}
#create long data frame for plotting, including only the relevant variables
plotd_m <- data_males %>% select(CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% pivot_longer(cols=sexdrive2:attr, names_to="var", values_to="score")
#create a table with the quantiles for each variable
quant_m <- as.data.frame(t(do.call(rbind.data.frame, (lapply(c("CSBD", "lon", "meffort", "mvalue", "PPCS_6", "sexdrive2", "socialanx"), function(x){parse_number(gsub(",.*$", "", levels(cut_number(as.data.frame(data_males)[, x], 4))))})))))
#rename columns
colnames(quant_m) <- c("CSBD", "lon", "meffort", "mvalue", "PPCS_6", "sexdrive2", "socialanx")
#pivot for plotting
quant_m <- pivot_longer(quant_m, cols=CSBD:socialanx, names_to='var')
#call on function in other R script
source("./functions/flat_violin.R")
#plot
g1 <- ggplot(plotd_m %>% filter(var!="attr"), aes(x=as.factor(var), y=score, fill=var)) +
  geom_flat_violin(scale="width") +
  scale_fill_manual(values=c("#444444", "#4d4d4d", "#666666", "#999999", "#b3b3b3", "#bfbfbf", "#cccccc")) +
  geom_dotplot(binaxis="y", dotsize=.04, stackdir="down", binwidth=.3, position=position_nudge(-.025)) +
  geom_errorbar(data=quant_m, aes(x=as.factor(var), ymin=value, ymax=value, linetype="quantiles"), inherit.aes=F, linewidth=.5, width=.3, position=position_nudge(x=.15)) +
  scale_linetype_manual(values="solid") +
  guides(linetype=guide_legend(""), fill="none") +
  labs(x="", y="value") +
  theme_classic()
#plot attraction to children separately
g2 <- ggplot(plotd_m %>% filter(var=="attr"), aes(x=score)) +
  geom_bar() +
  theme_classic() +
  labs(x="attraction to children", y="count")
#arrange plots
ggarrange(g1, g2, ncol=2, nrow=1, widths=c(1.6, 1))
```

__Figure Y__

*Scale Distributions With Quantiles for Female Participants*
```{r plot women, fig.width=12}
#create long data frame for plotting, including only the relevant variables
plotd_f <- data_females %>% select(CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% pivot_longer(cols=sexdrive2:attr, names_to="var", values_to="score")
#create a table with the quantiles for each variable (except CSBD & PPCS_6)
quant_f <- as.data.frame(t(do.call(rbind.data.frame, (lapply(c("lon", "meffort", "mvalue", "sexdrive2", "socialanx"), function(x){parse_number(gsub(",.*$", "", levels(cut_number(as.data.frame(data_females)[, x], 4))))})))))
#rename columns
colnames(quant_f) <- c("lon", "meffort", "mvalue", "sexdrive2", "socialanx")
#pivot for plotting
quant_f <- pivot_longer(quant_f, cols=lon:socialanx, names_to='var')
#add CSBD & PPCS_6 manually because they can only be dichotomised
quant_f <- rbind(quant_f, pivot_longer(as.data.frame(t(do.call(rbind.data.frame, (lapply(c("CSBD", "PPCS_6"), function(x){parse_number(gsub(",.*$", "", levels(cut_number(as.data.frame(data_females)[, x], 2))))}))))), cols=V1:V2, names_to='var'))
#rename variables
quant_f$var[quant_f$var=="V1"] <- "CSBD"
quant_f$var[quant_f$var=="V2"] <- "PPCS_6"
#call on function in other R script
source("./functions/flat_violin.R")
#plot
g3 <- ggplot(plotd_f %>% filter(var!="attr"), aes(x=as.factor(var), y=score, fill=var)) +
  geom_flat_violin(scale="width") +
  scale_fill_manual(values=c("#444444", "#4d4d4d", "#666666", "#999999", "#b3b3b3", "#bfbfbf", "#cccccc")) +
  geom_dotplot(binaxis="y", dotsize=.04, stackdir="down", binwidth=.3, position=position_nudge(-.025)) +
  geom_errorbar(data=quant_f, aes(x=as.factor(var), ymin=value, ymax=value, linetype="quantiles"), inherit.aes=F, linewidth=.5, width=.3, position=position_nudge(x=.15)) +
  scale_linetype_manual(values="solid") +
  guides(linetype=guide_legend(""), fill="none") +
  labs(x="", y="value") +
  theme_classic()
#plot attraction to children separately
g4 <- ggplot(plotd_f %>% filter(var=="attr"), aes(x=score)) +
  geom_bar() +
  theme_classic() +
  labs(x="attraction to children", y="count")
#arrange plots
ggarrange(g3, g4, ncol=2, nrow=1, widths=c(1.6, 1))
#clean up
rm(g1)
rm(g2)
rm(g3)
rm(g4)
```

Descriptives for the newly categorised scales are shwon in Table X.
```{r factor, eval=F}
#We factorise and bin the variables in the data frame, such that we hopefully get enough cases per bin to estimate parameters in the LCA. Attr is dichotomised, so that we have at least some participants in the cells. However, attr is so severely skewed that we get only around 50 participants who answer non-zero - that will likely produce problems with the LCA. 
m_factor <- lapply(data_males %>% select(-CASE, -attr),cut_number, n=4) %>% as.tibble()
m_factor <- lapply(m_factor, ordered) %>% as.tibble()
data_males <- cbind(data_males$CASE, m_factor, data_males$attr) %>% mutate(attr  = case_when(`data_males$attr` == 0 ~ "no attraction", `data_males$attr` > 0 ~ "some attraction"))
data_males <- data_males %>% select(-`data_males$attr`)
data_males <- data_males %>% mutate(attr = attr %>% ordered())
colnames(data_males)[1]<-"CASE"

#Clean up
rm(m_factor)

#We redo the re-coding for the female data. Be reminded that the cuts for the quantiles are likely different between males and females - keep that in mind while interpreting the LCAs!
f_factor <- lapply(data_females %>% select(-CASE, -attr, -CSBD, -PPCS_6),cut_number, n=4) %>% as.tibble()
f_factor <- lapply(f_factor, ordered) %>% as.tibble()
data_females <- cbind(data_females$CASE, f_factor, data_females$attr) %>% mutate(attr  = case_when(`data_females$attr` == 0 ~ "no attraction", `data_females$attr` > 0 ~ "some attraction"), CSBD = data_females$CSBD, PPCS_6 = data_females$PPCS_6)
data_females <- data_females %>% select(-`data_females$attr`)
data_females <- data_females %>% mutate(attr = attr %>% ordered())
colnames(data_females)[1]<-"CASE"

#Dichotomise CSBD, PPCS_6 because they have not enough cases per bin to cluster it in quantiles. The names of the factor levels show where the cut was set.
data_females <- data_females %>% mutate(PPCS_6 = cut_number(data_females$PPCS_6, n=2) %>% ordered(), CSBD = cut_number(data_females$CSBD, n=2) %>% ordered())

#Clean up
rm(f_factor)

#Descriptives for males
desc_m <- tidySEM::descriptives(data_males %>% select(-CASE))
desc_m <- desc_m %>% select(name, type, n, missing, unique, mode, mode_value, v)

#Descriptives for females
desc_f <- tidySEM::descriptives(data_females %>% select(-CASE))
desc_f <- desc_f %>% select(name, type, n, missing, unique, mode, mode_value, v)
```

__Table X__

*Descriptives for the Newly Categorised Scales*
```{r descriptives}
#Create vector with rownames
Variable <- c("Sexual interest in children", "CSBD-7", "LON", "Mating effort", "Low embodied capital", "PPCS-6", "Sex drive", "Social anxiety")
#Merge male and female descriptives tables
desc <- merge(desc_m, desc_f, by="name")
#Bind Variable column to the begninning
desc <- cbind(Variable, desc)

#Pretty output
flextable(desc, col_keys=c("Variable", "n.x", "n.y", "unique.x", "unique.y", "mode.x", "mode.y", "mode_value.x", "mode_value.y", "v.x", "v.y")) %>%
  set_header_labels(Variable="Scale", n.x='Male', n.y="Female", unique.x="Male", unique.y="Female", mode.x="Male", mode.y="Female", mode_value.x="Male", mode_value.y="Female", v.x="Male", v.y ="Female") %>%
  add_header_row(values=c("Variable", "N", "Categories", "Mode", "Modal value", "v"), colwidths = c(1, 2, 2, 2, 2, 2)) %>%
  theme_booktabs() %>%
  autofit()
```
*Note.* Mode refers to how many participants share this modal value. v is a measure of variability for categorical variables and refers to the probability that two randomly drawn participants belong to different categories.

First we show males:
```{r show_desc_m}
desc_m
```

There are no missings, which is nice. With most variables we can observe that the quantile binning worked well (according to the v-values), except for the attraction to children variable. Even dichotomised too few few people report anything other than that they are completely unattracted to children. 

Then females: 
```{r show_desc_f}
desc_f
```

# Performing LCA

We begin with the data set comprised of males.

The steps include:  
  1. Class enumeration  
  2. Model evaluation  
  3. Model interpretation

## Males
```{r LCA_males, eval=FALSE, include=F}
#We start with constructing an LCA model for the people who reported a male gender identity. The consensus was to expect up to 7 clusters, because of the seven initial motivations and the original answering categories in the questionnaire. Very likely 7 clusters are way to many to reliably estimate parameters but we will see.

lca_models_m <- mx_lca(data=data_males %>% select(-CASE), classes=1:7)

#If non-convergence. Use mxTryHardOrdinal()

lca_fit <- table_fit(lca_models_m) %>% select(Name, LL, n, Parameters, BIC, Entropy, prob_min, prob_max, n_min, np_ratio, np_local)

#Entropy coefficient shows that the classes are not well separated. https://doi.org/10.1177/009579842093093 recommend a value of at least .8 and state that an LCA with an entropy coefficient lower than .6 will be hard to publish.

LR_lca_m <- lr_lmr(lca_models_m)

lca_final_model_m <- lca_models_m[[3]] #x is the class enumeration of choice

#Check results
table_LCA_m <- table_results(lca_final_model_m)

#conditional item probabilities
prob_table_LCA_m <- table_prob(lca_final_model_m)
prob_table_LCA_m <- reshape(prob_table_LCA_m, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))

#class proportions
class_prob_lca_m <- class_prob(lca_final_model_m, "sum.posterior")
```

We look at the BIC criterion, which is presented as the most reliable measure for class enumeration in LCA according to simulation studies [@van_lissa_recommended_2023; @masyn_latent_2013]. 
```{r scree_plot, echo=F}
plot(lca_fit)
```

There appears to be a minimum being reached at three classes. The following table shows us how many participants are in which class
```{r class proportion, echo=F}
#class proportions
class_prob_lca_m
```
<!-- We also tried to use Likelihood Ratio Tests for class enumeration, as they are often used together with fit indices such as the BIC, but encountered some problems. While the bootstrapped likelihood ratio test (which is sometimes recommended based on simulation studies) proved too computationally expensive for my home computer (with multiple days running time and no sign of the algorithm stopping), the Lo-Mendell-Rubin-LRT returned a warning and only NA values [@sinha_practitioners_2021; @van_lissa_recommended_2023]. The exact reason for this is still unclear. It might be that Vuong's distinguishability test, that is performed in the LMR-LRT procedure, came to the result that all models are actually indistinguishable [@merkle_testing_2016]. A solution with no clusters was also what was indicated by the OPTICS algorithm. We still need to verify the reason for the NA output. 
However, @van_lissa_recommended_2023 advise to be cautious using LRTs, especially in exploratory LCA, because the conditions underlying the LRT are likely not met and the test results may be flawed or, as is the case with BLRTs, they depend heavily on the model specifications and should better be used in confirmatory settings; see also @jeffries_note_2003. -->

We did not calculate a likelihood ratio test (LRT), as @van_lissa_recommended_2023 advise to be cautious using LRTs. This especially true in exploratory LCA because the conditions underlying the LRT are likely not met and the test results may be flawed or, as is the case with bootstrapped LRTs, they depend heavily on the model specifications and should better be used in confirmatory settings; see also @jeffries_note_2003.

First, we inspect the fit_table to assess how well the models with different numbers of classes perform in that data set. We should always keep in mind that the fit indices give us only in-sample fit and should therefore be taken with a grain of salt. 
```{r show_fit_table_m, echo=FALSE}
lca_fit
```

__Inverse entropy__ is a measure of high class separability. Values close to 1 are desirable. @weller_latent_2020 refer to a cut_off value of at least .8 and comment that, although no agreed-upon cut-off value for entropy exists, one will have problems publishing a solution with entropy values as low as .6. @nylund-gibson_ten_2018 also refer to a cut-off value of .8, but see @clark2009relating. __Class separability__ is often inversely related to model fit, and is not used in model selection. However, it should still be a relevant criterion for model evaluation, as it affects assessment of the usefulness of the model - what use are classes that cannot be well discerned @masyn_latent_2013?. Values around our values are associated with a misclassification rate of around 20% for the assignment to the correct class @wang2017performance. This insecurity can project into further analyses with the latent classes, so that higher inverse entropy values lead to higher replication of correct effects of class membership on distal outcomes @clark2009relating. In our case the problematic values for class separability might relate to differences between how well the different scales can distinguish between the classes in the sample. That warrants a closer inspection of the posterior item probabilities.

__Prob_min__ and __Prob_max__ give the range of the posterior classification probabilities. These values should be high.
```{r avePP_m, echo=F}
class_prob(lca_final_model_m, type="avg.mostlikely")
```

The __average posterior probabilities by most likely class membership__ tell how high the posterior probabilities are on average for all individuals whose highest posterior probability points to a specific class. The off-diagonal can be viewed as a crude measure of misclassification error. 
@masyn_latent_2013 suggest a .7 as a cut-off on the diagonal of the average posterior probabilities by most likely class membership matrix. @nylund-gibson_ten_2018 recommend .7 as well (via Nagin, 2005). However, @van_lissa_recommended_2023 remark that it is still necessary to extensively interpret the resulting matrix, no matter what cut-off values suggest. To that end, we would recommend looking at the whole distributions of posterior probabilities by most likely class instead of their first moment.

Let's display conditional item probabilities for the three class model for men. The table can help identify which items are useful for the separation of the classes and the probabilities themselves are a measure for class homogeneity.
```{r prob_table_m, echo=F}
prob_table_LCA_m
```

__Class homogeneity__ and __class separation__ might be explored by the item probabilities conditioned on class membership (conditional item probabilities); in our case these are rather memberships to quantiles of the item scales than the usual dummy coded items. Probabilities higher than .7 and smaller than .3 show homogeneity within the classes. Odds ratios between two classes bigger than 5 and smaller than .2 would indicate high class separation for an item - or in our case an item level @nylund-gibson_ten_2018.

__np_ratio__ tells us how many observations on average are accessible for each parameter estimation [@van_lissa_recommended_2023]. __np_local__ refers to the number of cases per parameter in the smallest class and indicates how much data we have for each parameter.

The conditional item probabilities can also be inspected in a plot. In the end they are what is used in the interpretation of the results and labelling of classes.
```{r prob_plot_m, echo=F, fig.width=12}
plot_prob(lca_final_model_m)
```

## Females

Now we redo the analysis with the subset of female participants. The cutoff values for fit indices obviously stay the same. It should be noted that two more variables needed to be dichotomised in the female sample, because they were also to skewed to form quantiles. Those were PPCS and CSBD.
```{r LCA_females, eval=FALSE, include=F}
#We carry on with a LCA on the female sample. We stay with a maximum of seven clusters.

lca_models_f <- mx_lca(data=data_females %>% select(-CASE), classes=1:7)

#If non-convergence. Use mxTryHardOrdinal()

lca_fit_f <- table_fit(lca_models_f) %>% select(Name, LL, n, Parameters, BIC, Entropy, prob_min, prob_max, n_min, np_ratio, np_local)

#Entropy coeffient shows that the classes are not well separated. https://doi.org/10.1177/009579842093093 recommend a value of at least .8 and state that an LCA with an entropy coefficient lower than .6 will be har to publish.

LR_lca_f <- lr_lmr(lca_models_f)

lca_final_model_f <- lca_models_f[[3]] #x is the class enumaration of choice

#Check tge 
table_LCA_f <- table_results(lca_final_model_f)

prob_table_LCA_f <- table_prob(lca_final_model_f)
prob_table_LCA_f <- reshape(prob_table_LCA_f, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))

#class proportions
class_prob_lca_f <- class_prob(lca_final_model_f, "sum.posterior")
```

### Scree Plot Females
```{r scree_plot_f, echo=FALSE}
plot(lca_fit_f)
```

### Class Proportions Females
```{r class_proportion_f, echo=F}
#class proportions
class_prob_lca_f
```

### Fit Table Females
```{r showfittable_f, echo=FALSE}
lca_fit_f
```

### AvePP by Most Likely Class for Females
```{r avePP_f, echo=FALSE}
class_prob(lca_final_model_f, type="avg.mostlikely")
```

### Conditional Item Probabilities for Females
```{r prob_table_f, echo=FALSE}
prob_table_LCA_f
```

```{r prob_plot_f, echo=FALSE, fig.width=12}
plot_prob(lca_final_model_f)
```


*References*