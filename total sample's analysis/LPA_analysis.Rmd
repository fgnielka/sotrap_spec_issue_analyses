---
title: "LPA analysis"
output: word_document
---

```{r setup, include=F, message=F, warning=F}
knitr::opts_chunk$set(echo=F, warning=F, message=F)

set.seed(42)

#Declare vector of required packages
packages <- c("tidyverse", "readr", "mclust")

#load function to check whether required packages are installed.
source("./functions/check_required_packages.R")
check_required_packages(packages)

#load required packages
lapply(packages, require, character.only=T)

##import results

#Results from the Latent Profile Analysis of the whole sample is imported from the saved RDS files. 
lpa_full_data <- readRDS("./out/LPA_full_data.RDS")

#All lpa models in all cv-folds. Including the data used for fitting in the model objects.
lpa_models <- readRDS("./out/lpa_models.RDS")

#cross-validated clustering coefficients including DBCV.
lpa_model_eval <- readRDS("./out/lpa_model_eval.RDS")

#Results from the OPTICS clustering. All models in all cv folds
optics_models <- readRDS("./out/optics_models.RDS")

#load data
data <- haven::read_spss("./data/dataE_clear5 2209.sav") %>% select(CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr)

colnames(data) <- c("uid", "sex_drive", "csbd", "ppcs", "soi_r_me", "social_anxiety", "lon", "mvs", "attraction_to_children")
```

# Outliers

```{r outlier_detection, eval=F}
###Distribution plots####
dens_plots <- list()
##sex drive
dens_plots[["sex_drive"]]<-ggplot(data, aes(x=sex_drive)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

##csbd
dens_plots[["csbd"]]<-ggplot(data, aes(x=csbd)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

##ppcs
dens_plots[["ppcs"]]<-ggplot(data, aes(x=ppcs)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

##soi-r_me
dens_plots[["soi_r_me"]]<-ggplot(data, aes(x=soi_r_me)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

##social anxiety
dens_plots[["social_anxiety"]]<-ggplot(data, aes(x=social_anxiety)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

##lon
dens_plots[["lon"]]<-ggplot(data, aes(x=lon)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

##mvs
dens_plots[["mvs"]]<-ggplot(data, aes(x=mvs)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

##attraction to children
dens_plots[["attraction_to_children"]]<-ggplot(data, aes(x=attraction_to_children)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")

dens_plots[["dens_grid"]]<-ggpubr::ggarrange(
          dens_plots[["sex_drive"]],
          dens_plots[["csbd"]],
          dens_plots[["ppcs"]],
          dens_plots[["soi_r_me"]],
          dens_plots[["mvs"]],
          dens_plots[["social_anxiety"]],
          dens_plots[["lon"]],
          dens_plots[["mvs"]],
          dens_plots[["attraction_to_children"]],
          ncol=5,
          nrow=2
          )
```

# Cross-Validation Preparation

```{r create_folds, eval=F}
#We create folds fÃ¼r 5-fold 10 times repeated cross validation. Caret performs stratified CV by default. The returned vector gives the rownumber of the training cases.
folds <- caret::createMultiFolds(data %>% pull(uid), k=5, times=10)
```

```{r hyperparameter_tuning, eval=F}
#We use cross validation to find the optimal number of clusters in lpa. We form a grid of 3 to 7 clusters which are the cluster numbers we expect theoretically. The models are added to a tibble - the lpa models object.
lpa_models <- list()
G <- 3:7

for (g in G){
  lpa_models[[as.character(g)]] <- lapply(folds, function(x) {Mclust(data = data[x,] %>% select(-uid), prior=priorControl(), G=g, modelNames=c("VVV"))}) 
}

#the create the tibble and flatten it to avoid working with hierarchical lists.
lpa_models <- tibble(names = lpa_models %>% list_flatten() %>% names(), models = lpa_models %>% list_flatten())

#We create a tidier representation of to what fold, repetition cycle a model belongs and how many clusters were fitted.
names <- lpa_models %>% pull(names) %>% str_split_fixed("_", 2) %>% as_tibble() %>% rename(nclust=V1, fold=V2) 

names <- names %>% select(-fold) %>% cbind( names %>% pull(fold) %>% str_split_fixed("\\.", 2) %>% as_tibble() %>% rename(fold = V1, rep = V2))

lpa_models <- cbind(names, lpa_models)
lpa_models <- lpa_models %>% select(-names)

#We add the folds used for trainig to the tibble to have all the information concisely stored in one place.
lpa_models <- lpa_models %>% mutate(folds=replicate(5, folds, simplify = FALSE) %>% list_flatten())

#We need to create cluster predictions to evaluate the out of sample performance of the clustering. The predictions are first stored in lpa_evaluation and are then added to lpa_models. The uids of the cases that were predicted in each test prediction can be found in cv_predicted_clusters.
lpa_evaluation <- list()

for (i in 1:nrow(lpa_models)){
  
  model <- lpa_models$models[[i]]
  newdata <- data[setdiff(1:nrow(data), lpa_models$folds[[i]]),] %>% select(-uid)
  lpa_evaluation[[i]] <- predict.Mclust(model, newdata)$classification
}

test_folds <- list()

for (i in 1:nrow(lpa_models)){
  test_folds[[i]] <- data[setdiff(1:nrow(data), lpa_models$folds[[i]]),] %>% pull(uid)
}

lpa_models <- lpa_models %>% mutate(test_fold_uids = test_folds, cv_predicted_clusters = lpa_evaluation)

#tidy up behind us
rm(fold, folds, lpa_evaluation, model, names, newdata, test_folds)

#We compute the silhouette index on our cross validation sets.
#We compute the silhouette index for each fold. 
lpa_models <- lpa_models %>% mutate(cv_silhouette = lapply(1:nrow(lpa_models), function(x){
  ClusterR::silhouette_of_clusters(data[data$uid %in% (lpa_models[x,c("test_fold_uids")] %>% unlist()),] %>% select(-uid), lpa_models[x,] %>% pull(cv_predicted_clusters) %>% unlist())$silhouette_global_average
}) %>% unlist()) 

#We create the Dunn Index for each fold as well. And we even choose two different metrics: Euclidean distance and Cosine dissimilarity, for more specific comparison. Dunn index has a minimum of 0 and a maximum of 1 - should be maximized.
source("./functions/Cosine Similarity.R")

#Computes the dunn index with cosine similarity instead of euclidean distances.
lpa_models <- lpa_models %>% mutate(cv_dunn_cos = lapply(1:nrow(lpa_models),function(x){
  #Get a distance measure to base the Dunn index on. Cosine is nice, because the scale differences are less important as with euclidean distances.
  distance <- data[data$uid %in% (lpa_models[x,c("test_fold_uids")] %>% unlist()),] %>% select(-uid) %>% cos_dissim
  #Get predicted cluster memberships
  clusters <- (lpa_models[x,] %>% pull(cv_predicted_clusters) %>% unlist())
  #feed both in the dunn function from the clValid package
  return(clValid::dunn(distance=distance, clusters=clusters))
}) %>% unlist())

#Computes the dunn index based on euclidean distances
lpa_models <- lpa_models %>% mutate(cv_dunn = lapply(1:nrow(lpa_models),function(x){
  #Get a distance measure to base the Dunn index on. Cosine is nice, because the scale differences are less important as with euclidean distances.
  distance <- data[data$uid %in% (lpa_models[x,c("test_fold_uids")] %>% unlist()),] %>% select(-uid)
  #Get predicted cluster memberships
  clusters <- (lpa_models[x,] %>% pull(cv_predicted_clusters) %>% unlist())
  #feed both in the dunn function from the clValid package
  return(clValid::dunn(Data=distance, clusters=clusters, method = "euclidean"))
}) %>% unlist())

#Last but not least, we compute the Connectivty index for the clusters. Using the same logic, once for euclidean distances, once for cosine dissimilarities.Connectivity should be near 0. 
#creating connectivity index based on cosine dissimilarity
lpa_models <- lpa_models %>% mutate(cv_conn_cos = lapply(1:nrow(lpa_models),function(x){
  #Get a distance measure to base the connectivity index on. Cosine is nice, because the scale differences are less important as with euclidean distances. 
  distance <- data[data$uid %in% (lpa_models[x,c("test_fold_uids")] %>% unlist()),] %>% select(-uid) %>% cos_dissim
  #Get predicted cluster memberships
  clusters <- (lpa_models[x,] %>% pull(cv_predicted_clusters) %>% unlist())
  #feed both in the dunn function from the clValid package
  return(clValid::connectivity(distance=distance, clusters=clusters))
}) %>% unlist())

#creating Connectivity index for euclidean distances
lpa_models <- lpa_models %>% mutate(cv_conn = lapply(1:nrow(lpa_models),function(x){
  #Get a distance measure to base the Dunn index on. Cosine is nice, because the scale differences are less important as with euclidean distances.
  distance <- data[data$uid %in% (lpa_models[x,c("test_fold_uids")] %>% unlist()),] %>% select(-uid)
  #Get predicted cluster memberships
  clusters <- (lpa_models[x,] %>% pull(cv_predicted_clusters) %>% unlist())
  #feed both in the dunn function from the clValid package
  return(clValid::connectivity(Data=distance, clusters=clusters, method = "euclidean"))
}) %>% unlist())

lpa_model_eval <- lpa_models %>% group_by(nclust) %>% summarize(conn_cos = mean(cv_conn_cos), conn = mean(cv_conn), dunn = mean(cv_dunn), dunn_cos = mean(cv_dunn_cos), sil=mean(cv_silhouette))
```

```{r data_export, eval=F}
results <- lpa_models %>% select(nclust, fold, rep, test_fold_uids, cv_predicted_clusters)

results <- results  %>% unite("dat_name", nclust:fold:rep, sep="_", remove=FALSE) %>% select(-nclust, -fold,-rep) 

#Here we paste all predicted cluster memberships of all folds in seperate csv files and put them in the data prepared folder. From there the DBCV script can grab them and compute a DBCV value for each of the 
for (i in 1:nrow(results)){
  t <- tibble(uid = results[i,] %>% pull(test_fold_uids) %>% unlist(), cluster = results[i,] %>% pull(cv_predicted_clusters) %>% unlist()) %>% left_join(data, by=join_by(uid == uid))
  name <- paste("./data/data_prepared/",results[i,c("dat_name")],".csv", sep="")
  readr::write_csv(x=t, file=name)
}
```

```{r insert_dbcv, eval=F}
dbcv <- readr::read_csv("./data/LPA_cv_predicted_clusters/dbcv_dat.csv") %>% as.tibble() %>% select(dbcv)

lpa_models <- lpa_models %>% mutate("dbcv" = dbcv)

#Insert the cross-validated clustering coefficients. For some reason the column "dbcv" does not show as such, but as "dbcv$dbcv", but that seams to me to be just a cosmetic problem. 
lpa_model_eval <- lpa_model_eval %>% mutate(dbcv = lpa_models %>% group_by(nclust) %>% summarize(dbcv = mean(dbcv$dbcv, na.rm=T)) %>% select(dbcv))
```

# Latent Profile Analysis

```{r lpa, eval=F}
#That is the initial code for modelling the LPAs. Only in-sample fit is used to determine numbers of clusters. The resulting BICS appeared pretty close to one another, so that I got suspicious.  

choose_BIC <- mclustBIC(data %>% select(-uid), prior = priorControl())
plot(choose_BIC)

lpa_models <- list()

for (i in 3:7){
  lpa_models[[as.character(i)]] <- Mclust(data %>% select(-uid), prior=priorControl(), G=i, modelNames=c("VVV"))
}
```



