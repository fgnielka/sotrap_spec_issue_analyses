---
title: "LCA & BCH"
output: word_document
bibliography: bibliography.bib
---

```{r setup & library, include=F, message=F, warning=F}
#Do not show code chunks in the knitted document
knitr::opts_chunk$set(echo=F, warning=F, message=F)

#For reproducible results
set.seed(100)

#Turn off scientific notification
options(scipen=999)

#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM", "MOTE", "flextable", "logistf")

#Load function to check whether required packages are installed & load required packages
source("./functions/check_required_packages.R")
check_required_packages(packages)
lapply(packages, require, character.only=T)

#Clean up
rm(packages)

#######ATTENTION!!!
#At this point we load a pre-saved .RData file in which the environment is saved. That makes the code less reproducible, but removing the eval=F specifications in all code blocks will make that possible 
load(".RData")
```

# **Latent Class Analysis**

<!--We begin by loading the required data set ("dataE_2112.sav") into R. The selected variables are: gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr

The data set is split into one data frame for men and one for women, which are analysed separately, after the initial approach (i.e. investigating both genders at once with an LPA and OPTICS) did not result in an interpretable solution.

The performed analyses are exploratory. -->

```{r data, include=F}
#We use gender instead of sex. A 2 represents a male. We want to only grab males. 
data_males <- haven::read_spss("./data/data raw/dataE_2112.sav") %>% dplyr::select(gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% filter(gender == 2) %>% dplyr::select(-gender)
```

After the initial approach (i.e. investigating both genders at once with an LPA and OPTICS) did not result in any interpretable solutions, only participants who identified as male were analysed from here on (for the analyses with the female participants, see the supplemental material). Because of the highly skewed distributions of some of the variables, we decided to split the scales into quantiles (see Figure A), and, when sufficient data to split the data into quantiles was not available, to dichotomise them. This was the case for sexual interest in children, where 97% of the participants completely denied sexual interest in children (i.e., selected 0 on all three items). The descriptives for the newly categorised scales are shown in Table A.

```{r factor, eval=F}
#We factorise and bin the variables in the data frame, such that we hopefully get enough cases per bin to estimate parameters in the LCA. Attr is dichotomised, so that we have at least some participants in the cells. However, attr is so severely skewed that we get only around 50 participants who answer non-zero - that will likely produce problems with the LCA. 
m_factor <- lapply(data_males %>% select(-CASE, -attr), cut_number, n=4) %>% as_tibble()
m_factor <- lapply(m_factor, ordered) %>% as_tibble()
data_males <- cbind(data_males$CASE, m_factor, data_males$attr) %>% mutate(attr  = case_when(`data_males$attr` == 0 ~ "no attraction", `data_males$attr` > 0 ~ "some attraction"))
data_males <- data_males %>% select(-`data_males$attr`)
data_males <- data_males %>% mutate(attr = attr %>% ordered())
colnames(data_males)[1]<-"CASE"

#Clean up
rm(m_factor)

#Descriptives for males
desc_m <- tidySEM::descriptives(data_males %>% select(-CASE))
desc_m <- desc_m %>% select(name, type, n, missing, unique, mode, mode_value, v)
```

**Table A**

*Descriptives for the Newly Categorised Scales*

```{r descriptives}
# #Create vector with rownames
# Variable <- c("Sexual interest in children", "CSBD-7", "LON", "Mating effort", "Mate value", "PPCS-6", "Sex drive", "Social anxiety")
# #Merge male and female descriptives tables
# desc <- merge(desc_m, desc_f, by="name")
# #Bind Variable column to the beginning
# desc <- cbind(Variable, desc)

#Create vector with rownames
Scale <- c("Attraction to children", "Compulsive sex.", "Loneliness", "Mating effort", "Mate value", "Probl. porn use", "Sex drive", "Social anxiety")
#Bind Variable column to the beginning
desc_m <- cbind(Scale, desc_m[, -1])

#Pretty output
flextable(desc_m %>% select(-"type", -"missing")) %>%
  set_formatter(v=function(x) format(round(x, 2), nsmall=2),
                unique=function(x) format(round(x-1, 0), nsmall=0)) %>%
  set_header_labels(n="N", unique="Categories", mode="Mode", mode_value="Modal value", v="v") %>%
  theme_booktabs() %>%
  autofit()
```

*Note.* Mode refers to how many participants share this modal value. *v* is a measure of variability for categorical variables and refers to the probability that two randomly drawn participants belong to different categories. Round parentheses are exclusive boundaries, square brackets are inclusive boundaries.

<!--There are no missings, which is nice. With most variables we can observe that the quantile binning worked well (according to the v-values), except for the attraction to children variable. Even dichotomised too few few people report anything other than that they are completely unattracted to children. 

# Performing LCA

We begin with the data set comprised of males.

The steps include:  
  1. Class enumeration  
  2. Model evaluation  
  3. Model interpretation

## Males -->

```{r LCA_males, eval=F, include=F}
#We start with constructing an LCA model for the people who reported a male gender identity. The consensus was to expect up to 7 clusters, because of the seven initial motivations and the original answering categories in the questionnaire. Very likely 7 clusters are way to many to reliably estimate parameters but we will see.

lca_models_m <- mx_lca(data=data_males %>% select(-CASE), classes=1:7)
lca_models_m_1 <- read_rds("./out/lca_models_m_2.1.RDS")
lca_models_m_2 <- read_rds("./out/lca_models_m_2.2.RDS")
lca_models_m <- c(lca_models_m_1, lca_models_m_2)

#If non-convergence. Use mxTryHardOrdinal()

lca_fit <- table_fit(lca_models_m) %>% select(Name, LL, n, Parameters, BIC, Entropy, prob_min, prob_max, n_min, np_ratio, np_local)

#Entropy coefficient shows that the classes are not well separated. https://doi.org/10.1177/009579842093093 recommend a value of at least .8 and state that an LCA with an entropy coefficient lower than .6 will be hard to publish.

LR_lca_m <- lr_lmr(lca_models_m)

lca_final_model_m <- lca_models_m[[3]] #x is the class enumeration of choice

#Check results
table_LCA_m <- table_results(lca_final_model_m)

#conditional item probabilities
prob_table_LCA_m <- table_prob(lca_final_model_m)
prob_table_LCA_m <- reshape(prob_table_LCA_m, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))

#class proportions
class_prob_lca_m <- class_prob(lca_final_model_m, "sum.posterior")
```

Models with one to seven classes were estimated. For class enumeration, the Bayesian information criterion (BIC) is, according to simulation studies, the most reliable measure [@van_lissa_recommended_2023; @masyn_latent_2013]. It reached a minimum at three classes (see Figure B). Following Van Lissa et al. (2023), a likelihood ratio test (LRT) was not calculated to avoid biased results due to unmet conditions (see also @jeffries_note_2003). Instead, fit indices were inspected to assess how well the different models performed.

**Figure B**

*BIC Values for Different Numbers of Classes*

```{r scree_plot}
plot(lca_fit)
```

<!-- We also tried to use Likelihood Ratio Tests for class enumeration, as they are often used together with fit indices such as the BIC, but encountered some problems. While the bootstrapped likelihood ratio test (which is sometimes recommended based on simulation studies) proved too computationally expensive for my home computer (with multiple days running time and no sign of the algorithm stopping), the Lo-Mendell-Rubin-LRT returned a warning and only NA values [@sinha_practitioners_2021; @van_lissa_recommended_2023]. The exact reason for this is still unclear. It might be that Vuong's distinguishability test, that is performed in the LMR-LRT procedure, came to the result that all models are actually indistinguishable [@merkle_testing_2016]. A solution with no clusters was also what was indicated by the OPTICS algorithm. We still need to verify the reason for the NA output. 
However, @van_lissa_recommended_2023 advise to be cautious using LRTs, especially in exploratory LCA, because the conditions underlying the LRT are likely not met and the test results may be flawed or, as is the case with BLRTs, they depend heavily on the model specifications and should better be used in confirmatory settings; see also @jeffries_note_2003.

We did not calculate a likelihood ratio test (LRT), as @van_lissa_recommended_2023 advise to be cautious using LRTs. This especially true in exploratory LCA because the conditions underlying the LRT are likely not met and the test results may be flawed or, as is the case with bootstrapped LRTs, they depend heavily on the model specifications and should better be used in confirmatory settings; see also @jeffries_note_2003.

First, we inspect the fit_table to assess how well the models with different numbers of classes perform in that data set. We should always keep in mind that the fit indices give us only in-sample fit and should therefore be taken with a grain of salt. -->

Different fit indices for model evaluation are displayed in Table B. Inverse entropy as a measure of class separability was below the often used target minimum value of .80 for all models (e.g., @nylund-gibson_ten_2018; Weller et al., 2020). These results are associated with a misclassification rate of around 20% for the assignment to the correct class (Wang et al., 2017) and might relate to differences between how well the different scales can distinguish between the classes in the sample. Of all models with more than one class, the three-class solution scored the highest value (see Table B).

**Table B**

*Fit Indices for Different Numbers of Classes*

```{r show_fit_table_m}
flextable(lca_fit) %>%
  set_formatter(Entropy=function(x) format(round(x, 2), nsmall=2),
                prob_min=function(x) format(round(x, 2), nsmall=2),
                prob_max=function(x) format(round(x, 2), nsmall=2),
                n_min=function(x) format(round(x, 2), nsmall=2),
                np_ratio=function(x) format(round(x, 2), nsmall=2),
                np_local=function(x) format(round(x, 2), nsmall=2)) %>%
  set_header_labels(Name="Number of classes", LL="LogLikelihood", n="N", prob_min="Min prob", prob_max="Max prob", n_min="Min n", np_ratio="NP ratio", np_local="Local NP"
                    ) %>%
  theme_booktabs() %>%
  autofit()
```

*Note.* Min prob and Min max are the minimum and maximum values (respectively) on the diagonal of the table of average posterior probabilities by most likely class membership. The higher these values, the better the model fit (recommended cut-off = .7; @masyn_latent_2013; @nylund-gibson_ten_2018). Min n is the proportion of participants that are assigned to the smallest class. NP ratio shows how many observations are accessible for each parameter estimation on average @van_lissa_recommended_2023. Local NP refers to the number of cases per parameter in the smallest class and indicates how much data there are for each parameter.

<!--__Inverse entropy__ is a measure of high class separability. Values close to 1 are desirable. @weller_latent_2020 refer to a cut_off value of at least .8 and comment that, although no agreed-upon cut-off value for entropy exists, one will have problems publishing a solution with entropy values as low as .6. @nylund-gibson_ten_2018 also refer to a cut-off value of .8, but see @clark2009relating. __Class separability__ is often inversely related to model fit, and is not used in model selection. However, it should still be a relevant criterion for model evaluation, as it affects assessment of the usefulness of the model - what use are classes that cannot be well discerned @masyn_latent_2013?. Values around our values are associated with a misclassification rate of around 20% for the assignment to the correct class @wang2017performance. This insecurity can project into further analyses with the latent classes, so that higher inverse entropy values lead to higher replication of correct effects of class membership on distal outcomes @clark2009relating. In our case the problematic values for class separability might relate to differences between how well the different scales can distinguish between the classes in the sample. That warrants a closer inspection of the posterior item probabilities.

__Prob_min__ and __Prob_max__ give the range of the posterior classification probabilities. These values should be high.

```{r avePP_m, echo=F}
class_prob(lca_final_model_m, type="avg.mostlikely")
```
 
The __average posterior probabilities by most likely class membership__ tell how high the posterior probabilities are on average for all individuals whose highest posterior probability points to a specific class. The off-diagonal can be viewed as a crude measure of misclassification error. 
@masyn_latent_2013 suggest a .7 as a cut-off on the diagonal of the average posterior probabilities by most likely class membership matrix. @nylund-gibson_ten_2018 recommend .7 as well (via Nagin, 2005). However, @van_lissa_recommended_2023 remark that it is still necessary to extensively interpret the resulting matrix, no matter what cut-off values suggest. To that end, we would recommend looking at the whole distributions of posterior probabilities by most likely class instead of their first moment. -->

According to the BIC values, the model with three classes showed the best fit, while the fit indices for model evaluation confirmed that the fit was mostly acceptable, so that it was chosen for interpretation.

The largest class is class 1 with 43.4% of all male participants (*n* = 548), followed by class 3 with 28.6% (*n* = 361), and class 2 with 28.0% (*n* = 354).

<!--Table C shows how many participants are in which class
```{r class proportion, echo=F}
class_prob_lca_m
```
-->

Table C displays conditional item probabilities for the three-class model, which are a measure for class homogeneity, with probabilities \> .7 and \< .3 showing homogeneity within the classes. This can help identify which items are useful for the separation of the classes.

**Table C**

*Conditional Item Probabilities for the Three-Class Model*

```{r prob_table_m, echo=F}
#Make table presentable
prob_table_LCA_m$Variable <- c(rep(c("Sex drive", "Compulsive sex.", "Probl. porn use", "Mating effort", "Social anxiety", "Loneliness", "Mate value", "Attraction to children"), each=4))[1:nrow(prob_table_LCA_m)]
#Pretty display
flextable(prob_table_LCA_m) %>%
  set_formatter(Probability.class1=function(x) format(round(x, 2), nsmall=2),
                Probability.class2=function(x) format(round(x, 2), nsmall=2),
                Probability.class3=function(x) format(round(x, 2), nsmall=2)) %>%
  set_header_labels(Probability.class1="Class 1", Probability.class2="Class 2", Probability.class3="Class 3") %>%
  theme_booktabs() %>%
  autofit()
```

<!--__Class homogeneity__ and __class separation__ might be explored by the item probabilities conditioned on class membership (conditional item probabilities); in our case these are rather memberships to quantiles of the item scales than the usual dummy coded items. Probabilities higher than .7 and smaller than .3 show homogeneity within the classes. Odds ratios between two classes bigger than 5 and smaller than .2 would indicate high class separation for an item - or in our case an item level @nylund-gibson_ten_2018.-->

Figure C shows the conditional item probabilities in a more accessible way.

**Figure C**

*Conditional Item Probabilities per Class*

```{r prob_plot_m, fig.width=12}
plot_prob(lca_final_model_m) +
  scale_fill_grey(start = 0.9, end = 0.5, aesthetics = "fill") +
  scale_x_discrete(labels=c("attr"="Attraction to children", "CSBD"="Compulsive sex.", "lon"="Loneliness", "meffort"="Mating effort", "mvalue"="Mate value", "PPCS_6"="Probl. porn use", "sexdrive2"="Sex drive", "socialanx"="Social anxiety")) +
  labs(x="") +
  theme_classic() +
  theme(axis.text.x=element_text(angle=65, hjust=1, size=10))
```

# **Distal Outcomes Analyses**

To test whether class membership is related to the proclivity for sexually deviant behaviours, distal outcomes analyses were performed with all single proclivity items.

Since class membership is measured imperfectly by a LCA, the assignment of participants to a class is associated with a certain amount of error. If this error is left unaccounted for and predicted class membership from a LCA is used as a predictor in subsequent analyses, bias is introduced. Multiple approaches have been proposed, by which the error associated with class assignment is taken into account. The so-called three-step approach will first fit an independent measurement model (here, the LCA without any covariates or outcomes), assign participants to latent classes, and then use the predicted classes as estimates for the latent classes (Bakk & Kuha, 2021). The important difference between naive and recommended three-step approaches is the employment of countermeasures at the second or third step to avoid the bias resulting from imperfect class assignments.

The approach chosen here is known as modified Bolck-Croon-Hagenaars (BCH) approach and was developed by Bakk et al. (2013), building on and generalising the work of Vermunt et al. (2012). In this approach, complex sample weights are obtained for the participants and used in the estimation of the target model in ensuing analyses. However, the weighing resulted in negative weights, which is not uncommon when using the modified BCH approach (e.g. Vermunt, 2010), but can cause problems in the presence of low class separability, as found in the present analysis.

<!--The original data frame is modified, so that each participant is assigned to each possible class (i.e. each participant appears class-times in the analysis) and is weighted according to the probability of the class assignment each time they are entered in the analysis and a likelihood is computed.

In our specific case, the weighing resulted in negative weights for each row in which a participant was not assigned to the class for which they had the largest posterior probability (as given by the independent LCA model). That is something that is not unlikely to happen. Multiple authors have reported of problems that can arise from such negative sample weights in the presence of low class separability (XXX, XXX). These often refer to negative variances in the outcome variables in SEM (Bakk et al., 2013). -->

Firth's logistic regression with intercept correction (FLIC) was chosen as a method of analysis for the dichotomised proclivity items, as the base rate for many of these items was extremely low and standard logistic regression is known to either underestimate the base rate of a rare outcome or to produce impossible regression weights (XXX). According to simulation studies, FLIC out performs other methods to correct logistic results (XXX).

Even though FLIC allows for sample weights, the negative weights led to convergence problems. Therefore, we fixed all negative weights to a value close to zero (here, .01), which is also close to their original values. Thus, the weights for participants where they were assigned to any other class than their modal class assignment would still enter the analysis, but would not influence the estimation much. <!-- The original class weights can still be viewed in the BCH_expanded_dataframe, and it can be seen that although they were not-zero, they were still much closer to zero than the weights for the modal class assignments. My hope is, that the ad-hoc adjustment will therefore not change the estimations much. But it is nothing that we can be sure of. It is known, that analysis using BCH-adjusted samples produce underestimation of standard errors, which can lead to too progressive decisions in significance tests. Correction methods exist for the application in SEM contexts (XXX), but the immediate application for our analysis method is unclear.-->This will have introduced some bias in the regression weights and standard errors; however, it remains uncertain to which degree our inference statistics are affected by this. <!--The exploratory nature of our analysis probably validates not to further explore correction methods for inference statistics. Yet, we should be more cautious than ever in the interpretation of significant p-values close to the cut-off values. That is of course, in general a good practice but is now more important than usual. -->

```{r BCH_dataframe, eval=F}
#Load precoded functions
source("./functions/D_matrix_modal.R")
source("./functions/modal_weights.R")

#Creating a dataframe with modal weights
#First, extract posterior probabilities for class assignment and most likely class membership
posteriors <- class_prob(lca_final_model_m, type="individual")$individual %>% as_tibble()

#Compute the inverse D matrix as in Bakk et al. (2013)
inverse_D <- solve(D_matrix_modal(posteriors))

#Extract a vector with the most likely class memberships
modal_assignments <- posteriors %>% select(predicted)

#Use the modal_weights function to create modal weights from the inverse_D matrix and combine the results in a tibble with the posterior probabilities of class membership and the most likely class membership.
BCH_expanded_dataframe <- tibble(posteriors, modal_weights(modal_assignments, inverse_D) %>% as_tibble())
colnames(BCH_expanded_dataframe) <- c("postprob_class_1", "postprob_class_2", "postprob_class_3", "modal_class", "modal_weight_1", "modal_weight_2", "modal_weight_3")

#Clean up
rm(modal_assignments, inverse_D, posteriors)

#Attach the CASE variable to our weighted dataframe. 
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(CASE = data_males$CASE)

#We convert the expanded dataframe from wide in long format, so that we can use it as an input for our further analyses. According to Bakk et al. (2016), each participant enters the analysis nclass times, each time differently weighted according to his or her class assignment. 
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% pivot_longer(cols=starts_with("modal_weight"), values_to = "modal_weight", names_to=NULL)

#Create a variable that encodes the assumed class assignments for the BCH analysis
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(predicted_class = vctrs::vec_rep(1:3, nrow(data_males)))

#Create dummy variables from this variable
BCH_expanded_dataframe <- fastDummies::dummy_cols(BCH_expanded_dataframe, select_columns = "predicted_class")

#Combine the expanded dataframe with the outcome variables
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% full_join(y=haven::read_spss("./data/data raw/dataE_clear5 2209.sav") %>% filter(gender == 2) %>% select(CASE, starts_with("Y_") & ends_with("r")) %>% mutate(CASE = CASE %>% as.numeric()),by="CASE")
```

Significance of regression weights was tested using a penalised LRT, as implemented by the R package logistf (Heinze et al., 2023), which was also used to estimate the FLIC models. To make the estimation for the model with the proclivity to have sex with a prostitute item possible, the iteratively weighted least squares method instead of the standard Newton-Raphson approach had to be used for optimisation.

```{r BCH_logreg, eval=F}
#The Regression estimation appears to be not working with negative weights. I do not understand why this is neccessarily the case. We approach to ad-hoc solutions: 1. Fixing the negative weights to zero, 2. We fix the negative weights to 0.01. 

#Fixing negative weights to zero or nigh-zero
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(modal_weight_2 = case_when(BCH_expanded_dataframe$modal_weight < 0 ~ 0, .default = BCH_expanded_dataframe$modal_weight))
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(modal_weight_3 = case_when(BCH_expanded_dataframe$modal_weight < 0 ~ 0.01, .default = BCH_expanded_dataframe$modal_weight))

#We prepare the model specifications for the Firth Regression to run.
outcomes <- BCH_expanded_dataframe %>% select(starts_with("Y_")) %>% colnames()
predictors <- BCH_expanded_dataframe %>% select(starts_with("predicted")) %>% select(-predicted_class, -predicted_class_3) %>% colnames() %>% paste(collapse=" + ")
models <- paste(outcomes, "~", predictors, sep=" ")

#Do Flic (Firths regression with intercept correction) for the models in the list with weights fixed to zero.
log_regs <- list()
for (i in 1:17){
  log_regs[[i]] <- logistf::logistf(formula=models[i], data = BCH_expanded_dataframe, model=TRUE, weights=BCH_expanded_dataframe$modal_weight_2) %>% flic()
}

##Do Flic for the models in the list with weights fixed to 0.01
log_regs_min <- list()
for (i in 1:17){
  log_regs_min[[i]] <- logistf::logistf(formula=models[i], data = BCH_expanded_dataframe,control = logistf.control(fit="IRLS"), model=TRUE, weights=BCH_expanded_dataframe$modal_weight_3) %>% flic()
}

#Enter Extra Analysis for people who assumed young persons to be of an age range that is potentially problematic, i.e. <15. Extra analysis was inspired by information that people consuming tend to call victims young persons. 
CASE_yp <-  haven::read_spss("./data/data raw/dataE_2112.sav") %>% filter(gender == 2)  %>% filter(range_young1 < 15) %>% dplyr::select(CASE) %>% unlist()

BCH_expanded_dataframe %>% filter(CASE %in% CASE_yp) %>% select(contains("Youth"))

Yp_variables <- BCH_expanded_dataframe %>% select(contains("Yp")) %>% colnames()
Yp_models <- paste(Yp_variables, "~", predictors, sep=" ")


logregs_yp <- lapply(Yp_models, function(x){
  logistf(x,data=BCH_expanded_dataframe %>% filter(CASE %in% CASE_yp),
          weights=BCH_expanded_dataframe[BCH_expanded_dataframe$CASE %in% CASE_yp,]$modal_weight_2) %>%
    flic()
})

logregs_yp_0.01 <- lapply(Yp_models, function(x){
  logistf(x,data=BCH_expanded_dataframe %>% filter(CASE %in% CASE_yp),
          weights=BCH_expanded_dataframe[BCH_expanded_dataframe$CASE %in% CASE_yp,]$modal_weight_3) %>%
    flic()
})

######Adapt Code for the extra analyses 
#Set up a dataframe for the the outputs of the regression analysis. Each row corresponds to one item.
BCH_results <- tibble(outcomes = c(colnames(BCH_expanded_dataframe %>% select(starts_with("Y_"))),paste(Yp_variables, "_red", sep="")))

#Combine the results in a dataframe
BCH_results <- BCH_results %>% mutate(models = c(log_regs,logregs_yp))
BCH_results <- BCH_results %>% mutate(models_0.01_weights = c(log_regs_min,logregs_yp_0.01))

#Because of the case weights we have to test the parameters manually via likelihood ratio tests
BCH_results <- BCH_results %>% mutate(
lr_1 = lapply(BCH_results$models, logistftest, test=1, weights=BCH_expanded_dataframe$modal_weight_2),
lr_2 = lapply(BCH_results$models, logistftest, test=2, weights=BCH_expanded_dataframe$modal_weight_2),
lr_3 = lapply(BCH_results$models, logistftest, test=3, weights=BCH_expanded_dataframe$modal_weight_2),
lr_total = lapply(BCH_results$models, logistftest, weights=BCH_expanded_dataframe$modal_weight_2))

BCH_results <- BCH_results %>% mutate(
lr_1_0.01 = lapply(BCH_results$models, logistftest, test=1, weights=BCH_expanded_dataframe$modal_weight_3,control = logistf.control(fit="IRLS")),
lr_2_0.01 = lapply(BCH_results$models, logistftest, test=2, weights=BCH_expanded_dataframe$modal_weight_3,control = logistf.control(fit="IRLS")),
lr_3_0.01 = lapply(BCH_results$models, logistftest, test=3, weights=BCH_expanded_dataframe$modal_weight_3,control = logistf.control(fit="IRLS")),
lr_total_0.01 = lapply(BCH_results$models, logistftest, weights=BCH_expanded_dataframe$modal_weight_3,control = logistf.control(fit="IRLS")))

#We extract p-values from the likelihood ratio tests. 
BCH_results <- BCH_results %>% mutate(
  p_1=lapply(BCH_results$lr_1, function(x){x$prob}) %>% unlist(),
  p2=lapply(BCH_results$lr_2, function(x){x$prob}) %>% unlist(),
  p3=lapply(BCH_results$lr_3, function(x){x$prob}) %>% unlist(),
  p_1_0.01=lapply(BCH_results$lr_1_0.01, function(x){x$prob}) %>% unlist(),
  p_2_0.01=lapply(BCH_results$lr_2_0.01, function(x){x$prob}) %>% unlist(),
  p_3_0.01=lapply(BCH_results$lr_3_0.01, function(x){x$prob}) %>% unlist(),
)

#Order BCH_results
BCH_results <- BCH_results[order(BCH_results$outcomes),]

#Add parameter estimates
#How to index the results_parameters table:
#All regression weights start with "b",
#All model output from the 0.01 weighted regression ends with "_0.01"
#All lower confidence interval limits begin with ci_l
#All upper confidence interval limits begin with ci_r
#All lr-test p-values start with p_lr.
#Use tidyverse select functions 

BCH_results_parameters <-tibble(outcomes = BCH_results$outcomes, 
                                b_intercept = lapply(BCH_results$models, function(x){x$coefficients[1]}) %>% unlist(),
                                b_predicted_class_1 = lapply(BCH_results$models, function(x){x$coefficients[2]}) %>% unlist(),
                                b_predicted_class_2 =lapply(BCH_results$models, function(x){x$coefficients[3]}) %>% unlist(),
                                b_intercept_0.01 = lapply(BCH_results$models_0.01_weights, function(x){x$coefficients[1]}) %>%
                                  unlist(),
                                b_predicted_class_1_0.01 = lapply(BCH_results$models_0.01_weights, function(x){x$coefficients[2]}) %>%
                                  unlist(),
                                b_predicted_class_2_0.01 = lapply(BCH_results$models_0.01_weights, function(x){x$coefficients[3]}) %>%
                                  unlist(),
                                ci_l_intercept = lapply(BCH_results$models, function(x){x$ci.lower[1]}) %>% unlist(),
                                ci_l_predicted_class_1 =lapply(BCH_results$models, function(x){x$ci.lower[2]}) %>% unlist(),
                                ci_l_predicted_class_2 =lapply(BCH_results$models, function(x){x$ci.lower[3]}) %>% unlist(),
                                ci_l_intercept_0.01= lapply(BCH_results$models_0.01_weights, function(x){x$ci.lower[1]}) %>% unlist(),
                                ci_l_predicted_class_1_0.01=lapply(BCH_results$models_0.01_weights, function(x){x$ci.lower[2]}) %>%
                                  unlist(),
                                ci_l_predicted_class_2_0.01=lapply(BCH_results$models_0.01_weights, function(x){x$ci.lower[3]}) %>%
                                  unlist(),
                                ci_r_intercept =lapply(BCH_results$models, function(x){x$ci.upper[1]}) %>% unlist(),
                                ci_r_predicted_class_1=lapply(BCH_results$models, function(x){x$ci.upper[2]}) %>% unlist(),
                                ci_r_predicted_class_2=lapply(BCH_results$models, function(x){x$ci.upper[3]}) %>% unlist(),
                                ci_r_intercept_0.01 = lapply(BCH_results$models_0.01_weights, function(x){x$ci.upper[1]}) %>% unlist(),
                                ci_r_predicted_class_1_0.01 = lapply(BCH_results$models_0.01_weights, function(x){x$ci.upper[2]}) %>%
                                  unlist(),
                                ci_r_predicted_class_2_0.01  = lapply(BCH_results$models_0.01_weights, function(x){x$ci.upper[3]}) %>%
                                  unlist(),
                                p_lr_intercept = lapply(BCH_results$lr_1, function(x){x$prob}) %>% unlist(),
                                p_lr_predicted_class_1 = lapply(BCH_results$lr_2, function(x){x$prob}) %>% unlist(),
                                p_lr_predicted_class_2 = lapply(BCH_results$lr_3, function(x){x$prob}) %>% unlist(),
                                p_lr_intercept_0.01 = lapply(BCH_results$lr_1_0.01, function(x){x$prob}) %>% unlist(),
                                p_lr_predicted_class_1_0.01 = lapply(BCH_results$lr_2_0.01, function(x){x$prob}) %>% unlist(),
                                p_lr_predicted_class_2_0.01 = lapply(BCH_results$lr_3_0.01, function(x){x$prob}) %>% unlist(),
                                p_lr_total = lapply(BCH_results$lr_total, function(x){x$prob}) %>% unlist(),
                                p_lr_total_0.01 = lapply(BCH_results$lr_total_0.01, function(x){x$prob}) %>% unlist())

#Save output to RDS.
#BCH_results %>% saveRDS(file="./out/BCH_results.RDS")

#Save Parameters Table to RDS
#BCH_results_parameters %>% saveRDS(file="./out/BCH_results_parameters.RDS")
```

<!--Each model (either with negative weights = 0 or negative weights = 0.01) can be obtained in the BCH_results tibble. The results from likelihood ratio tests are found in the columns that begin with "lr_". 1 means that the intercept is tested against the full model, 2 means that predicted class 1 is tested against the full model, 3 means that predicted class 2 is tested against the full model, and total means that the complete model was tested against an intercept only model. The regression weights can be interpreted as the difference in logits (log-odds of the probability to not answer 0 on the proclivity items) between the high risk class and the other classes. In other words, the significance of a regression weight tells us that there is a significant difference between the high risk class and the other classes when it comes to proclivity.-->

The predicted class 3 was chosen as the reference class because it contained the participants with the highest probabilities of being in high quantiles of the motivator scales. It can therefore be considered a "high risk"-class. Table D shows the odds and odds ratios (ORs) of the different models and classes. The odds indicate how much more likely it is for a member of the high- risk class to choose any answer other than zero on the respective proclivity items (e.g. participants in the-high risk class are 0.08 times more likely to show some proclivity to chat with a child). ORs indicate the change in odds for the respective class 1 or 2 compared to the high-risk class to choose any answer other than zero on the respective proclivity items (e.g. participants in class 1 have 0.20 times the odds of showing some proclivity to chat with a child compared to the high-risk group). Since CIs were obtained through a different method (Wald's test statistic as is the default in the logistf package) than the p values (which were obtained via penalised LRTs), significance decisions between the two methods can differ. For the present analysis, the LRTs should be relied on because they are based on fewer assumptions than the Wald's test statistic. The CIs should therefore interpreted with caution, as is indicated by our distal outcomes method.

```{r show_BCH_results}
#Put relevant results in a table
results <- cbind(Proclivity = c("Flirting or having sexual conversations via chat or webcam with a child", #1
                                "Flirting or having sexual conversations via chat or webcam with a young person", #2
                                "Driving under the influence of drugs/alcohol", #3
                                "Paying or giving gifts to a child for online sexual material (for example, videos, images, or online streaming)", #4
                                "Paying or giving gifts to a young person for online sexual material (for example, videos, images, or online streaming", #5
                                "Killing someone", #6
                                "Watching porn depicting a child", #7
                                "Watching porn depicting a child on the Darknet", #8
                                "Watching porn depicting a young person", #9
                                "Engaging in sexual activity with a prostitute", #10
                                "Watching porn in public spaces such as a bus or library", #11
                                "Engaging in sexual activity with an adult who does not agree or is not able to agree, for example, due to intoxication", #12
                                "Robbing a bank", #13
                                "Having offline sex or sexual contact with a child", #14
                                "Having offline sex or sexual contact with a young person", #15
                                "Driving faster than the posted speed limit", #16
                                "Engaging in sexual activity with an animal"), #17
                 BCH_results_parameters %>% 
                   filter(!str_detect(outcomes, "_red")) %>% #we do not want to show the additional analyses for the young persons after all
                   # filter(!outcomes %in% c("Y_proclChatYp_r", "Y_proclGiftYp_r", "Y_proclPornYp_r", "Y_proclSexYp_r")) %>% 
                 select(ends_with("total_0.01"), starts_with("b") & ends_with("0.01"), starts_with("p") & ends_with("0.01")
                        #, starts_with("ci") & ends_with("0.01")
                        ))
# #Create extra columns with CIs
# results$CI_intercept <- paste0("[", format(round(exp(results$ci_l_intercept_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_intercept_0.01), 2), nsmall=2), "]")
# results$CI_class1 <- paste0("[", format(round(exp(results$ci_l_predicted_class_1_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_1_0.01), 2), nsmall=2), "]")
# results$CI_class2 <- paste0("[", format(round(exp(results$ci_l_predicted_class_2_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_2_0.01), 2), nsmall=2), "]")
#Add odds for each class
results <- results %>% mutate(odds_predicted_class_1_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_1_0.01), 
                              odds_predicted_class_2_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_2_0.01),
                              odds_predicted_class_3_0.01 = exp(results$b_intercept_0.01))
#Change order of rows for unity with other tables
results <- results[c(10, 11, 12, 17, 7, 8, 1, 4, 14, 9, 2, 5, 15, 16, 3, 13, 6),]
#Pretty output
flextable(results, col_keys=c("Proclivity", "p_lr_total_0.01", "b_intercept_0.01", 
                              #"CI_intercept", 
                              "p_lr_intercept_0.01", "b_predicted_class_1_0.01", 
                              #"CI_class1", 
                              "p_lr_predicted_class_1_0.01", "b_predicted_class_2_0.01", 
                              #"CI_class2", 
                              "p_lr_predicted_class_2_0.01")) %>% 
  set_formatter(p_lr_total_0.01=function(x) apa(x, decimals=3, leading=F), 
                b_intercept_0.01=function(x) format(round(exp(x), 2), nsmall=2), #transform into OR with exp()
                p_lr_intercept_0.01=function(x) apa(x, decimals=3, leading=F),
                b_predicted_class_1_0.01=function(x) format(round(exp(x), 2), nsmall=2), 
                p_lr_predicted_class_1_0.01=function(x) apa(x, decimals=3, leading=F),
                b_predicted_class_2_0.01=function(x) format(round(exp(x), 2), nsmall=2), 
                p_lr_predicted_class_2_0.01=function(x) apa(x, decimals=3, leading=F)) %>%
  set_header_labels(p_lr_total_0.01="p", b_intercept_0.01="Odds", 
                    #CI_intercept="CI", 
                    p_lr_intercept_0.01="p", b_predicted_class_1_0.01="OR", 
                    #CI_class1="CI", 
                    p_lr_predicted_class_1_0.01="p", b_predicted_class_2_0.01="OR", 
                    #CI_class2="CI", 
                    p_lr_predicted_class_2_0.01="p") %>%
  add_header_row(values=c("", "Global LRT", "(Intercept)", "Class 1", "Class 2"), colwidths=c(1, 1, 2, 2, 2)) %>% 
  theme_booktabs() %>%
  autofit()
```

*Note.* Global LRT shows the *p* value of a test of the full against an intercept-only model. Penalised LRTs were used to obtain p values. Confidence intervals are not included because they would have been less reliable than *p* values (see above).

```{r plot_proclivities}
#rename proclivities for plotting
procl <- c("Prostitute", "Public porn", "Rape", "Zoophilia", "Porn child", "Porn child darknet", "Chat child", "Gift child", "Sex child", "Porn YP", "Chat YP", "Gift YP", "Sex YP", "Speed", "DUI", "Rob", "Kill")
#Rename proclivities
plot_results <- results
plot_results$Proclivity <- procl
#Factorise items
plot_results$Proclivity <- factor(plot_results$Proclivity, levels=plot_results$Proclivity)
#Pivot results table for plotting
#Only odds < 1 for better scaling
plot_results <- plot_results %>% 
  select(Proclivity, starts_with("odds")) %>% 
  filter(odds_predicted_class_1_0.01 < 1 & odds_predicted_class_2_0.01 < 1 & odds_predicted_class_3_0.01 < 1) %>% 
  pivot_longer(cols=starts_with("odds"), names_to="class", values_to="Odds") 
#Rename class column
plot_results$class <- rep(1:3, nrow(plot_results)/3)
#Remaining odds > 1
#Rename
plot_results_2 <- results
plot_results_2$Proclivity <- procl
#Factorise
plot_results_2$Proclivity <- factor(plot_results_2$Proclivity, levels=plot_results_2$Proclivity)
#Pivot
plot_results_2 <- plot_results_2 %>% 
  select(Proclivity, starts_with("odds")) %>% 
  filter(odds_predicted_class_1_0.01 > 1 | odds_predicted_class_2_0.01 > 1 | odds_predicted_class_3_0.01 > 1) %>% 
  pivot_longer(cols=starts_with("odds"), names_to="class", values_to="Odds") 
#Rename class column
plot_results_2$class <- rep(1:3, nrow(plot_results_2)/3)

#Line plots
#Odds < 1
ggplot(plot_results, aes(x=Proclivity, y=Odds, group=as.factor(class), linetype=as.factor(class))) +
  geom_line() +
  geom_point(aes(shape=as.factor(class))) +
  scale_linetype_manual(name=NULL, values=c("dotted", "dashed", "solid"), labels=c("Class 1", "Class 2", "Class 3")) +
  scale_shape_manual(name=NULL, values=c(3, 1, 0), labels=c("Class 1", "Class 2", "Class 3")) +
  theme_classic() +
  theme(axis.text.x=element_text(angle=45, hjust=1, size=8))
#Odds > 1
ggplot(plot_results_2, aes(x=Proclivity, y=Odds, group=as.factor(class), linetype=as.factor(class))) +
  geom_line() +
  geom_point(aes(shape=as.factor(class))) +
  scale_linetype_manual(name=NULL, values=c("dotted", "dashed", "solid"), labels=c("Class 1", "Class 2", "Class 3")) +
  scale_shape_manual(name=NULL, values=c(3, 1, 0), labels=c("Class 1", "Class 2", "Class 3")) +
  theme_classic() +
  theme(axis.text.x=element_text(angle=45, hjust=1, size=8))

# #Alternative: bar plots
# #Make a list of barplots with odds < 1
# plots <- lapply(plot_results$Proclivity %>% unique(), function(x){
#   ggplot(plot_results %>% filter(Proclivity == x), aes(x=class, y=odds, fill=as.factor(class))) +
#   scale_fill_manual(name=NULL, values=c("#CCCCCC", "#999999", "#666666")) +
#   geom_bar(stat="identity", position="dodge") +
#   scale_y_continuous(breaks=c(0.0, 0.5, 1.0), limits=c(0,1)) +
#   xlab("") +
#   ylab("") +
#   theme_classic() +
#   theme(legend.position="none")
# })
# #Make a list of barplots with odds > 1
# plots_2 <- lapply(plot_results_2$Proclivity %>% unique(), function(x){
#   ggplot(plot_results_2 %>% filter(Proclivity == x), aes(x=class, y=odds, fill=as.factor(class))) +
#   scale_fill_manual(name=NULL, values=c("#CCCCCC", "#999999", "#666666")) +
#   geom_bar(stat="identity", position="dodge") +
#   xlab("") +
#   ylab("") +
#   theme_classic() +
#   theme(legend.position="none")
#   })
# #Manually add scale labels to the "outer" plots
# plots[[1]] <- plots[[1]] + ylab("Odds")
# plots[[7]] <- plots[[7]] + ylab("Odds")
# plots[[1]] <- plots[[1]] + ylab("Odds")
# plots[[7]] <- plots[[7]] + ylab("Odds")
# plots[[12]] <- plots[[12]] + xlab("Class")
# plots[[13]] <- plots[[13]] + xlab("Class") + ylab("Odds")
# plots[[14]] <- plots[[14]] + xlab("Class")
# plots_2[[1]] <- plots_2[[1]] + xlab("Class")
# plots_2[[2]] <- plots_2[[2]] + xlab("Class")
# plots_2[[3]] <- plots_2[[3]] + xlab("Class")
# #Arrange all barplots in a grid
# ggarrange(plotlist=c(plots, plots_2), ncol=6, nrow=3)
```

*Note.* Public porn = "Watching porn in public spaces such as a bus or library", Rape = "Engaging in sexual activity with an adult who does not agree or is not able to agree, for example, due to intoxication", Zoophilia = "Engaging in sexual activity with an animal", Porn child = "Watching porn depicting a child", Porn child darknet = "Watching porn depicting a child on the Darknet", Chat child = "Flirting or having sexual conversations via chat or webcam with a child", Gift child = "Paying or giving gifts to a child for online sexual material (for example, videos, images, or online streaming)", Sex child = "Having offline sex or sexual contact with a child", Porn YP = "Watching porn depicting a young person", Chat YP = "Flirting or having sexual conversations via chat or webcam with a young person", Gift YP = "Paying or giving gifts to a young person for online sexual material (for example, videos, images, or online streaming", Sex YP = "Having offline sex or sexual contact with a young person", DUI = "Driving under the influence of drugs/alcohol", Kill = "Killing someone", Prostitute = "Engaging in sexual activity with a prostitute", Speed = "Driving faster than the posted speed limit", Rob = "Robbing a bank". Because of the high differences in odds, some of the items are displayed in a separate plot with a different scale.

<!-- We re-did the BCH analysis for young-person proclivity items only with the participants whose definition of a young person included people under 18.
```{r young_persons_analysis, eval=F}

CASE_yp <-  haven::read_spss("./data/data raw/dataE_2112.sav") %>% filter(gender == 2)  %>% filter(range_young1 < 15) %>% dplyr::select(CASE) %>% unlist()

BCH_expanded_dataframe %>% filter(CASE %in% CASE_yp) %>% select(contains("Youth"))

Yp_variables <- BCH_expanded_dataframe %>% select(contains("Yp")) %>% colnames()
Yp_models <- paste(Yp_variables, "~", predictors, sep=" ")


logregs_yp <- lapply(Yp_models, function(x){
  logistf(x,data=BCH_expanded_dataframe %>% filter(CASE %in% CASE_yp),
          weights=BCH_expanded_dataframe[BCH_expanded_dataframe$CASE %in% CASE_yp,]$modal_weight_2) %>%
    flic()
})

logregs_yp_0.01 <- lapply(Yp_models, function(x){
  logistf(x,data=BCH_expanded_dataframe %>% filter(CASE %in% CASE_yp),
          weights=BCH_expanded_dataframe[BCH_expanded_dataframe$CASE %in% CASE_yp,]$modal_weight_3) %>%
    flic()
})

#Put relevant results in a table
results <- cbind(Proclivity = c("Chat with a child", "Chat with a young person", "Chat with a young person [reduced]", "Drive under influence", "Make a gift to a child", "Make a gift to a young person", "Make a gift to a young person [reduced]", "Kill someone", "Watch CSEM", "Watch CSEM on the darknet", "Watch porn with a young person", "Watch porn with a young person [reduced]", "Have sex with a prostitute", "Watch porn in public", "Rape someone", "Rob a bank", "Have sex with a child", "Have sex with a young person", "Have sex with a young person [reduced]", "Drive over the speed limit", "Have sex with an animal"), BCH_results_parameters %>% 
                   # filter(!outcomes %in% c("Y_proclChatYp_r", "Y_proclGiftYp_r", "Y_proclPornYp_r", "Y_proclSexYp_r")) %>% 
                 select(ends_with("total_0.01"), starts_with("b") & ends_with("0.01"), starts_with("p") & ends_with("0.01"), starts_with("ci") & ends_with("0.01")))
#Create extra columns with CIs
results$CI_intercept <- paste0("[", format(round(exp(results$ci_l_intercept_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_intercept_0.01), 2), nsmall=2), "]")
results$CI_class1 <- paste0("[", format(round(exp(results$ci_l_predicted_class_1_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_1_0.01), 2), nsmall=2), "]")
results$CI_class2 <- paste0("[", format(round(exp(results$ci_l_predicted_class_2_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_2_0.01), 2), nsmall=2), "]")
#Add odds for each class
results <- results %>% mutate(odds_predicted_class_1_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_1_0.01), 
                              odds_predicted_class_2_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_2_0.01),
                              odds_predicted_class_3_0.01 = exp(results$b_intercept_0.01))
#Pretty output
flextable(results, col_keys=c("Proclivity", "p_lr_total_0.01", "b_intercept_0.01", "CI_intercept", "p_lr_intercept_0.01", "b_predicted_class_1_0.01", "CI_class1", "p_lr_predicted_class_1_0.01", "b_predicted_class_2_0.01", "CI_class2", "p_lr_predicted_class_2_0.01")) %>% 
  set_formatter(p_lr_total_0.01=function(x) apa(x, decimals=3, leading=F), 
                b_intercept_0.01=function(x) format(round(exp(x), 2), nsmall=2), #transform into OR with exp()
                p_lr_intercept_0.01=function(x) apa(x, decimals=3, leading=F),
                b_predicted_class_1_0.01=function(x) format(round(exp(x), 2), nsmall=2), 
                p_lr_predicted_class_1_0.01=function(x) apa(x, decimals=3, leading=F),
                b_predicted_class_2_0.01=function(x) format(round(exp(x), 2), nsmall=2), 
                p_lr_predicted_class_2_0.01=function(x) apa(x, decimals=3, leading=F)) %>%
  set_header_labels(p_lr_total_0.01="p", b_intercept_0.01="Odds", CI_intercept="CI", p_lr_intercept_0.01="p", b_predicted_class_1_0.01="OR", CI_class1="CI", p_lr_predicted_class_1_0.01="p", b_predicted_class_2_0.01="OR", CI_class2="CI", p_lr_predicted_class_2_0.01="p") %>%
  add_header_row(values=c("", "Global LRT", "(Intercept)", "Class 1", "Class 2"), colwidths=c(1, 1, 3, 3, 3)) %>% 
  theme_booktabs() %>%
  autofit()
```
-->

# **References**
