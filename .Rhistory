#Pretty output
flextable(results, col_keys=c("Proclivity", "p_lr_total_0.01", "b_intercept_0.01", "CI_intercept", "p_lr_intercept_0.01", "b_predicted_class_1_0.01", "CI_class1", "p_lr_predicted_class_1_0.01", "b_predicted_class_2_0.01", "CI_class2", "p_lr_predicted_class_2_0.01")) %>%
set_formatter(p_lr_total_0.01=function(x) apa(x, decimals=3, leading=F),
b_intercept_0.01=function(x) format(round(exp(x), 2), nsmall=2), #transform into OR with exp()
p_lr_intercept_0.01=function(x) apa(x, decimals=3, leading=F),
b_predicted_class_1_0.01=function(x) format(round(exp(x), 2), nsmall=2),
p_lr_predicted_class_1_0.01=function(x) apa(x, decimals=3, leading=F),
b_predicted_class_2_0.01=function(x) format(round(exp(x), 2), nsmall=2),
p_lr_predicted_class_2_0.01=function(x) apa(x, decimals=3, leading=F)) %>%
set_header_labels(p_lr_total_0.01="p", b_intercept_0.01="Odds", CI_intercept="CI", p_lr_intercept_0.01="p", b_predicted_class_1_0.01="OR", CI_class1="CI", p_lr_predicted_class_1_0.01="p", b_predicted_class_2_0.01="OR", CI_class2="CI", p_lr_predicted_class_2_0.01="p") %>%
add_header_row(values=c("", "Global LRT", "(Intercept)", "Class 1", "Class 2"), colwidths=c(1, 1, 3, 3, 3)) %>%
theme_booktabs() %>%
autofit()
results <- cbind(Proclivity = c("Flirting or having sexual conversations via chat or webcam with a child", "Flirting or having sexual conversations via chat or webcam with a young person", "Driving under the influence of drugs/alcohol", "Paying or giving gifts to a child for online sexual material (for example, videos, images, or online streaming)", "Paying or giving gifts to a young person for online sexual material (for example, videos, images, or online streaming", "Killing someone", "Watching porn depicting a child", "Watch porn depicting a child on the Darknet", "Watching porn depicting a young person", "Engaging in sexual activity with a prostitute", "Watching porn in public spaces such as a bus or library", "Engaging in sexual activity with an adult who does not agree or is not able to agree, for example, due to intoxication", "Robbing a bank", "Having offline sex or sexual contact with a child", "Having offline sex or sexual contact with a young person", "Driving faster than the posted speed limit", "Engaging in sexual activity with an animal"), BCH_results_parameters %>%
filter(!str_detect(outcomes, "_red")) %>% #we do not want to show the additional analyses for the young persons after all
# filter(!outcomes %in% c("Y_proclChatYp_r", "Y_proclGiftYp_r", "Y_proclPornYp_r", "Y_proclSexYp_r")) %>%
select(ends_with("total_0.01"), starts_with("b") & ends_with("0.01"), starts_with("p") & ends_with("0.01"), starts_with("ci") & ends_with("0.01")))
results$CI_intercept <- paste0("[", format(round(exp(results$ci_l_intercept_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_intercept_0.01), 2), nsmall=2), "]")
results$CI_class1 <- paste0("[", format(round(exp(results$ci_l_predicted_class_1_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_1_0.01), 2), nsmall=2), "]")
results$CI_class2 <- paste0("[", format(round(exp(results$ci_l_predicted_class_2_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_2_0.01), 2), nsmall=2), "]")
#Add odds for each class
results <- results %>% mutate(odds_predicted_class_1_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_1_0.01),
odds_predicted_class_2_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_2_0.01),
odds_predicted_class_3_0.01 = exp(results$b_intercept_0.01))
length(c(10, 11, 12, 17, 7, 8, 1, 4, 14, 9, 2, 5, 15, 16, 3, 13, 6))
#Put relevant results in a table
results <- cbind(Proclivity = c("Flirting or having sexual conversations via chat or webcam with a child", #1
"Flirting or having sexual conversations via chat or webcam with a young person", #2
"Driving under the influence of drugs/alcohol", #3
"Paying or giving gifts to a child for online sexual material (for example, videos, images, or online streaming)", #4
"Paying or giving gifts to a young person for online sexual material (for example, videos, images, or online streaming", #5
"Killing someone", #6
"Watching porn depicting a child", #7
"Watching porn depicting a child on the Darknet", #8
"Watching porn depicting a young person", #9
"Engaging in sexual activity with a prostitute", #10
"Watching porn in public spaces such as a bus or library", #11
"Engaging in sexual activity with an adult who does not agree or is not able to agree, for example, due to intoxication", #12
"Robbing a bank", #13
"Having offline sex or sexual contact with a child", #14
"Having offline sex or sexual contact with a young person", #15
"Driving faster than the posted speed limit", #16
"Engaging in sexual activity with an animal"), #17
BCH_results_parameters %>%
filter(!str_detect(outcomes, "_red")) %>% #we do not want to show the additional analyses for the young persons after all
# filter(!outcomes %in% c("Y_proclChatYp_r", "Y_proclGiftYp_r", "Y_proclPornYp_r", "Y_proclSexYp_r")) %>%
select(ends_with("total_0.01"), starts_with("b") & ends_with("0.01"), starts_with("p") & ends_with("0.01"), starts_with("ci") & ends_with("0.01")))
#Create extra columns with CIs
results$CI_intercept <- paste0("[", format(round(exp(results$ci_l_intercept_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_intercept_0.01), 2), nsmall=2), "]")
results$CI_class1 <- paste0("[", format(round(exp(results$ci_l_predicted_class_1_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_1_0.01), 2), nsmall=2), "]")
results$CI_class2 <- paste0("[", format(round(exp(results$ci_l_predicted_class_2_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_2_0.01), 2), nsmall=2), "]")
#Add odds for each class
results <- results %>% mutate(odds_predicted_class_1_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_1_0.01),
odds_predicted_class_2_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_2_0.01),
odds_predicted_class_3_0.01 = exp(results$b_intercept_0.01))
#Change order of rows for unity with other tables
results <- results[c(10, 11, 12, 17, 7, 8, 1, 4, 14, 9, 2, 5, 15, 16, 3, 13, 6),]
#Pretty output
flextable(results, col_keys=c("Proclivity", "p_lr_total_0.01", "b_intercept_0.01", "CI_intercept", "p_lr_intercept_0.01", "b_predicted_class_1_0.01", "CI_class1", "p_lr_predicted_class_1_0.01", "b_predicted_class_2_0.01", "CI_class2", "p_lr_predicted_class_2_0.01")) %>%
set_formatter(p_lr_total_0.01=function(x) apa(x, decimals=3, leading=F),
b_intercept_0.01=function(x) format(round(exp(x), 2), nsmall=2), #transform into OR with exp()
p_lr_intercept_0.01=function(x) apa(x, decimals=3, leading=F),
b_predicted_class_1_0.01=function(x) format(round(exp(x), 2), nsmall=2),
p_lr_predicted_class_1_0.01=function(x) apa(x, decimals=3, leading=F),
b_predicted_class_2_0.01=function(x) format(round(exp(x), 2), nsmall=2),
p_lr_predicted_class_2_0.01=function(x) apa(x, decimals=3, leading=F)) %>%
set_header_labels(p_lr_total_0.01="p", b_intercept_0.01="Odds", CI_intercept="CI", p_lr_intercept_0.01="p", b_predicted_class_1_0.01="OR", CI_class1="CI", p_lr_predicted_class_1_0.01="p", b_predicted_class_2_0.01="OR", CI_class2="CI", p_lr_predicted_class_2_0.01="p") %>%
add_header_row(values=c("", "Global LRT", "(Intercept)", "Class 1", "Class 2"), colwidths=c(1, 1, 3, 3, 3)) %>%
theme_booktabs() %>%
autofit()
#Do not show code chunks in the knitted document
knitr::opts_chunk$set(echo=F, warning=F, message=F)
#For reproducible results
set.seed(100)
#Turn off scientific notification
options(scipen=999)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM", "MOTE", "flextable", "logistf")
#Load function to check whether required packages are installed & load required packages
source("./functions/check_required_packages.R")
check_required_packages(packages)
lapply(packages, require, character.only=T)
#Clean up
rm(packages)
#######ATTENTION!!!
#At this point we load a pre-saved .RData file in which the environment is saved. That makes the code less reproducible, but removing the eval=F specifications in all code blocks will make that possible
load(".RData")
#Put relevant results in a table
results <- cbind(Proclivity = c("Flirting or having sexual conversations via chat or webcam with a child", #1
"Flirting or having sexual conversations via chat or webcam with a young person", #2
"Driving under the influence of drugs/alcohol", #3
"Paying or giving gifts to a child for online sexual material (for example, videos, images, or online streaming)", #4
"Paying or giving gifts to a young person for online sexual material (for example, videos, images, or online streaming", #5
"Killing someone", #6
"Watching porn depicting a child", #7
"Watching porn depicting a child on the Darknet", #8
"Watching porn depicting a young person", #9
"Engaging in sexual activity with a prostitute", #10
"Watching porn in public spaces such as a bus or library", #11
"Engaging in sexual activity with an adult who does not agree or is not able to agree, for example, due to intoxication", #12
"Robbing a bank", #13
"Having offline sex or sexual contact with a child", #14
"Having offline sex or sexual contact with a young person", #15
"Driving faster than the posted speed limit", #16
"Engaging in sexual activity with an animal"), #17
BCH_results_parameters %>%
filter(!str_detect(outcomes, "_red")) %>% #we do not want to show the additional analyses for the young persons after all
# filter(!outcomes %in% c("Y_proclChatYp_r", "Y_proclGiftYp_r", "Y_proclPornYp_r", "Y_proclSexYp_r")) %>%
select(ends_with("total_0.01"), starts_with("b") & ends_with("0.01"), starts_with("p") & ends_with("0.01")
#, starts_with("ci") & ends_with("0.01")
))
# #Create extra columns with CIs
# results$CI_intercept <- paste0("[", format(round(exp(results$ci_l_intercept_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_intercept_0.01), 2), nsmall=2), "]")
# results$CI_class1 <- paste0("[", format(round(exp(results$ci_l_predicted_class_1_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_1_0.01), 2), nsmall=2), "]")
# results$CI_class2 <- paste0("[", format(round(exp(results$ci_l_predicted_class_2_0.01), 2), nsmall=2), ", ", format(round(exp(results$ci_r_predicted_class_2_0.01), 2), nsmall=2), "]")
#Add odds for each class
results <- results %>% mutate(odds_predicted_class_1_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_1_0.01),
odds_predicted_class_2_0.01 = exp(results$b_intercept_0.01)*exp(results$b_predicted_class_2_0.01),
odds_predicted_class_3_0.01 = exp(results$b_intercept_0.01))
#Change order of rows for unity with other tables
results <- results[c(10, 11, 12, 17, 7, 8, 1, 4, 14, 9, 2, 5, 15, 16, 3, 13, 6),]
#Pretty output
flextable(results, col_keys=c("Proclivity", "p_lr_total_0.01", "b_intercept_0.01",
#"CI_intercept",
"p_lr_intercept_0.01", "b_predicted_class_1_0.01",
#"CI_class1",
"p_lr_predicted_class_1_0.01", "b_predicted_class_2_0.01",
#"CI_class2",
"p_lr_predicted_class_2_0.01")) %>%
set_formatter(p_lr_total_0.01=function(x) apa(x, decimals=3, leading=F),
b_intercept_0.01=function(x) format(round(exp(x), 2), nsmall=2), #transform into OR with exp()
p_lr_intercept_0.01=function(x) apa(x, decimals=3, leading=F),
b_predicted_class_1_0.01=function(x) format(round(exp(x), 2), nsmall=2),
p_lr_predicted_class_1_0.01=function(x) apa(x, decimals=3, leading=F),
b_predicted_class_2_0.01=function(x) format(round(exp(x), 2), nsmall=2),
p_lr_predicted_class_2_0.01=function(x) apa(x, decimals=3, leading=F)) %>%
set_header_labels(p_lr_total_0.01="p", b_intercept_0.01="Odds",
#CI_intercept="CI",
p_lr_intercept_0.01="p", b_predicted_class_1_0.01="OR",
#CI_class1="CI",
p_lr_predicted_class_1_0.01="p", b_predicted_class_2_0.01="OR",
#CI_class2="CI",
p_lr_predicted_class_2_0.01="p") %>%
add_header_row(values=c("", "Global LRT", "(Intercept)", "Class 1", "Class 2"), colwidths=c(1, 1, 2, 2, 2)) %>%
theme_booktabs() %>%
autofit()
View(results)
#Pivot results table for plotting
#Only odds < 1 for better scaling
plot_results <- results %>%
select(Proclivity, starts_with("odds")) %>%
filter(odds_predicted_class_1_0.01 < 1 & odds_predicted_class_2_0.01 < 1 & odds_predicted_class_3_0.01 < 1) %>%
pivot_longer(cols=starts_with("odds"), names_to="class", values_to="odds")
#Rename class column
plot_results$class <- rep(1:3, nrow(plot_results)/3)
#Remaining odds > 1
plot_results_2 <- results %>%
select(Proclivity, starts_with("odds")) %>%
filter(odds_predicted_class_1_0.01 > 1 | odds_predicted_class_2_0.01 > 1 | odds_predicted_class_3_0.01 > 1) %>%
pivot_longer(cols=starts_with("odds"), names_to="class", values_to="odds")
#Rename class column
plot_results_2$class <- rep(1:3, nrow(plot_results_2)/3)
#Line plots
#Odds < 1
ggplot(plot_results, aes(x=Proclivity, y=odds, group=as.factor(class), linetype=as.factor(class))) +
geom_line() +
geom_point(aes(shape=as.factor(class))) +
scale_linetype_manual(name=NULL, values=c("dotted", "dashed", "solid"), labels=c("Class 1", "Class 2", "Class 3")) +
scale_shape_manual(name=NULL, values=c(0,  3, 1)) +
guides(shape="none") +
theme_classic() +
theme(axis.text.x=element_text(angle=45, hjust=1, size=8))
#Odds > 1
ggplot(plot_results_2, aes(x=Proclivity, y=odds, group=as.factor(class), linetype=as.factor(class))) +
geom_line() +
geom_point(aes(shape=as.factor(class))) +
scale_linetype_manual(name=NULL, values=c("dotted", "dashed", "solid"), labels=c("Class 1", "Class 2", "Class 3")) +
scale_shape_manual(name=NULL, values=c(0,  3, 1)) +
guides(shape="none") +
theme_classic() +
theme(axis.text.x=element_text(angle=45, hjust=1, size=8))
# #Alternative: bar plots
# #Make a list of barplots with odds < 1
# plots <- lapply(plot_results$Proclivity %>% unique(), function(x){
#   ggplot(plot_results %>% filter(Proclivity == x), aes(x=class, y=odds, fill=as.factor(class))) +
#   scale_fill_manual(name=NULL, values=c("#CCCCCC", "#999999", "#666666")) +
#   geom_bar(stat="identity", position="dodge") +
#   scale_y_continuous(breaks=c(0.0, 0.5, 1.0), limits=c(0,1)) +
#   xlab("") +
#   ylab("") +
#   theme_classic() +
#   theme(legend.position="none")
# })
# #Make a list of barplots with odds > 1
# plots_2 <- lapply(plot_results_2$Proclivity %>% unique(), function(x){
#   ggplot(plot_results_2 %>% filter(Proclivity == x), aes(x=class, y=odds, fill=as.factor(class))) +
#   scale_fill_manual(name=NULL, values=c("#CCCCCC", "#999999", "#666666")) +
#   geom_bar(stat="identity", position="dodge") +
#   xlab("") +
#   ylab("") +
#   theme_classic() +
#   theme(legend.position="none")
#   })
# #Manually add scale labels to the "outer" plots
# plots[[1]] <- plots[[1]] + ylab("Odds")
# plots[[7]] <- plots[[7]] + ylab("Odds")
# plots[[1]] <- plots[[1]] + ylab("Odds")
# plots[[7]] <- plots[[7]] + ylab("Odds")
# plots[[12]] <- plots[[12]] + xlab("Class")
# plots[[13]] <- plots[[13]] + xlab("Class") + ylab("Odds")
# plots[[14]] <- plots[[14]] + xlab("Class")
# plots_2[[1]] <- plots_2[[1]] + xlab("Class")
# plots_2[[2]] <- plots_2[[2]] + xlab("Class")
# plots_2[[3]] <- plots_2[[3]] + xlab("Class")
# #Arrange all barplots in a grid
# ggarrange(plotlist=c(plots, plots_2), ncol=6, nrow=3)
#Load precoded functions
source("./functions/D_matrix_modal.R")
source("./functions/modal_weights.R")
#Creating a dataframe with modal weights
#First, extract posterior probabilities for class assignment and most likely class membership
posteriors_f <- class_prob(lca_final_model_f, type="individual")$individual %>% as_tibble()
#Compute the inverse D matrix as in Bakk et al. (2013)
inverse_D_f <- solve(D_matrix_modal(posteriors_f))
#Extract a vector with the most likely class memberships
modal_assignments_f <- posteriors_f %>% select(predicted)
#Use the modal_weights function to create modal weights from the inverse_D matrix and combine the results in a tibble with the posterior probabilities of class membership and the most likely class membership.
BCH_expanded_dataframe_f <- tibble(posteriors_f, modal_weights(modal_assignments_f, inverse_D_f) %>% as_tibble())
colnames(BCH_expanded_dataframe_f) <- c("postprob_class_1", "postprob_class_2", "postprob_class_3", "modal_class", "modal_weight_1", "modal_weight_2", "modal_weight_3")
#Clean up
rm(modal_assignments_f, inverse_D_f, posteriors_f)
#Attach the CASE variable to our weighted dataframe.
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(CASE = data_females$CASE)
#We convert the expanded data frame from wide in long format, so that we can use it as an input for our further analyses. According to Bakk et al. (2016), each participant enters the analysis nclass times, each time differently weighted according to her class assignment.
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% pivot_longer(cols=starts_with("modal_weight"), values_to = "modal_weight", names_to=NULL)
#Create a variable that encodes the assumed class assignments for the BCH analysis
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(predicted_class = vctrs::vec_rep(1:3, nrow(data_females)))
#Create dummy variables from this variable
BCH_expanded_dataframe_f <- fastDummies::dummy_cols(BCH_expanded_dataframe_f, select_columns = "predicted_class")
#Combine the expanded dataframe with the outcome variables
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% full_join(y=haven::read_spss("./data/data raw/dataE_clear5 2209.sav") %>% filter(gender == 1) %>% select(CASE, starts_with("Y_") & ends_with("r")) %>% mutate(CASE = CASE %>% as.numeric()), by="CASE")
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(modal_weight_2 = case_when(BCH_expanded_dataframe_f$modal_weight < 0 ~ 0, .default = BCH_expanded_dataframe_f$modal_weight))
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(modal_weight_3 = case_when(BCH_expanded_dataframe_f$modal_weight < 0 ~ 0.01, .default = BCH_expanded_dataframe_f$modal_weight))
outcomes_f <- BCH_expanded_dataframe_f %>% select(starts_with("Y_")) %>% colnames()
predictors_f <- BCH_expanded_dataframe_f %>% select(starts_with("predicted")) %>% select(-predicted_class, -predicted_class_3) %>% colnames() %>% paste(collapse=" + ")
models_f <- paste(outcomes_f, "~", predictors_f, sep=" ")
log_regs_f[[i]] <- logistf::logistf(formula=models_f[i], data = BCH_expanded_dataframe_f, model=TRUE, weights=BCH_expanded_dataframe_f$modal_weight_2) %>% flic()
log_regs_f <- list()
for (i in 1:17){
log_regs_f[[i]] <- logistf::logistf(formula=models_f[i], data = BCH_expanded_dataframe_f, model=TRUE, weights=BCH_expanded_dataframe_f$modal_weight_2) %>% flic()
}
log_regs_min_f <- list()
for (i in 1:17){
log_regs_min_f[[i]] <- logistf::logistf(formula=models_f[i], data = BCH_expanded_dataframe_f, control = logistf.control(fit="IRLS"), model=TRUE, weights=BCH_expanded_dataframe_f$modal_weight_3) %>% flic()
}
#Because of the case weights we have to test the parameters manually via likelihood ratio tests
BCH_results_f <- BCH_results_f %>% mutate(
lr_1 = lapply(BCH_results_f$models, logistftest, test=1,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_2 = lapply(BCH_results_f$models, logistftest, test=2,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_3 = lapply(BCH_results_f$models, logistftest, test=3,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_total = lapply(BCH_results_f$models, logistftest, weights=BCH_expanded_dataframe_f$modal_weight_2))
#Combine the results in a dataframe
BCH_results_f <- BCH_results_f %>% mutate(models = log_regs_f)
#Set up a dataframe for the the outputs of the regression analysis. Each row corresponds to one item.
BCH_results_f <- tibble(outcomes = c(colnames(BCH_expanded_dataframe_f %>% select(starts_with("Y_")))))
#Combine the results in a dataframe
BCH_results_f <- BCH_results_f %>% mutate(models = log_regs_f)
BCH_results_f <- BCH_results_f %>% mutate(models_0.01_weights = log_regs_min_f)
#Because of the case weights we have to test the parameters manually via likelihood ratio tests
BCH_results_f <- BCH_results_f %>% mutate(
lr_1 = lapply(BCH_results_f$models, logistftest, test=1,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_2 = lapply(BCH_results_f$models, logistftest, test=2,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_3 = lapply(BCH_results_f$models, logistftest, test=3,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_total = lapply(BCH_results_f$models, logistftest, weights=BCH_expanded_dataframe_f$modal_weight_2))
#Load precoded functions
source("./functions/D_matrix_modal.R")
source("./functions/modal_weights.R")
#Creating a dataframe with modal weights
#First, extract posterior probabilities for class assignment and most likely class membership
posteriors_f <- class_prob(lca_final_model_f, type="individual")$individual %>% as_tibble()
#Do not show code chunks in the knitted document
knitr::opts_chunk$set(echo=F, warning=F, message=F)
#For reproducible results
set.seed(100)
#Turn off scientific notification
options(scipen=999)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM", "MOTE", "flextable", "logistf")
#Load function to check whether required packages are installed & load required packages
source("./functions/check_required_packages.R")
check_required_packages(packages)
lapply(packages, require, character.only=T)
#Clean up
rm(packages)
#######ATTENTION!!!
#At this point we load a pre-saved .RData file in which the environment is saved. That makes the code less reproducible, but removing the eval=F specifications in all code blocks will make that possible
load(".RData")
##DATA##
#We use gender instead of sex. A 1 represents a female. We want to only grab females.
data_females <- haven::read_spss("./data/data raw/dataE_2112.sav") %>% dplyr::select(gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% filter(gender == 1) %>% dplyr::select(-gender)
#Load precoded functions
source("./functions/D_matrix_modal.R")
source("./functions/modal_weights.R")
#Creating a dataframe with modal weights
#First, extract posterior probabilities for class assignment and most likely class membership
posteriors_f <- class_prob(lca_final_model_f, type="individual")$individual %>% as_tibble()
#Compute the inverse D matrix as in Bakk et al. (2013)
inverse_D_f <- solve(D_matrix_modal(posteriors_f))
#Extract a vector with the most likely class memberships
modal_assignments_f <- posteriors_f %>% select(predicted)
#Use the modal_weights function to create modal weights from the inverse_D matrix and combine the results in a tibble with the posterior probabilities of class membership and the most likely class membership.
BCH_expanded_dataframe_f <- tibble(posteriors_f, modal_weights(modal_assignments_f, inverse_D_f) %>% as_tibble())
colnames(BCH_expanded_dataframe_f) <- c("postprob_class_1", "postprob_class_2", "postprob_class_3", "modal_class", "modal_weight_1", "modal_weight_2", "modal_weight_3")
#Clean up
rm(modal_assignments_f, inverse_D_f, posteriors_f)
#Attach the CASE variable to our weighted dataframe.
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(CASE = data_females$CASE)
#We convert the expanded data frame from wide in long format, so that we can use it as an input for our further analyses. According to Bakk et al. (2016), each participant enters the analysis nclass times, each time differently weighted according to her class assignment.
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% pivot_longer(cols=starts_with("modal_weight"), values_to = "modal_weight", names_to=NULL)
#Create a variable that encodes the assumed class assignments for the BCH analysis
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(predicted_class = vctrs::vec_rep(1:3, nrow(data_females)))
#Create dummy variables from this variable
BCH_expanded_dataframe_f <- fastDummies::dummy_cols(BCH_expanded_dataframe_f, select_columns = "predicted_class")
#Combine the expanded dataframe with the outcome variables
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% full_join(y=haven::read_spss("./data/data raw/dataE_clear5 2209.sav") %>% filter(gender == 1) %>% select(CASE, starts_with("Y_") & ends_with("r")) %>% mutate(CASE = CASE %>% as.numeric()), by="CASE")
#The Regression estimation appears to be not working with negative weights. I do not understand why this is neccessarily the case. We approach to ad-hoc solutions: 1. Fixing the negative weights to zero, 2. We fix the negative weights to 0.01.
#Fixing negative weights to zero or nigh-zero
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(modal_weight_2 = case_when(BCH_expanded_dataframe_f$modal_weight < 0 ~ 0, .default = BCH_expanded_dataframe_f$modal_weight))
BCH_expanded_dataframe_f <- BCH_expanded_dataframe_f %>% mutate(modal_weight_3 = case_when(BCH_expanded_dataframe_f$modal_weight < 0 ~ 0.01, .default = BCH_expanded_dataframe_f$modal_weight))
#We prepare the model specifications for the Firth Regression to run.
outcomes_f <- BCH_expanded_dataframe_f %>% select(starts_with("Y_")) %>% colnames()
predictors_f <- BCH_expanded_dataframe_f %>% select(starts_with("predicted")) %>% select(-predicted_class, -predicted_class_1) %>% colnames() %>% paste(collapse=" + ")
models_f <- paste(outcomes_f, "~", predictors_f, sep=" ")
#Do Flic (Firth's regression with intercept correction) for the models in the list with weights fixed to zero.
log_regs_f <- list()
for (i in 1:17){
log_regs_f[[i]] <- logistf::logistf(formula=models_f[i], data = BCH_expanded_dataframe_f, model=TRUE, weights=BCH_expanded_dataframe_f$modal_weight_2) %>% flic()
}
##Do Flic for the models in the list with weights fixed to 0.01
log_regs_min_f <- list()
for (i in 1:17){
log_regs_min_f[[i]] <- logistf::logistf(formula=models_f[i], data = BCH_expanded_dataframe_f, control = logistf.control(fit="IRLS"), model=TRUE, weights=BCH_expanded_dataframe_f$modal_weight_3) %>% flic()
}
#Set up a dataframe for the the outputs of the regression analysis. Each row corresponds to one item.
BCH_results_f <- tibble(outcomes = c(colnames(BCH_expanded_dataframe_f %>% select(starts_with("Y_")))))
#Combine the results in a dataframe
BCH_results_f <- BCH_results_f %>% mutate(models = log_regs_f)
BCH_results_f <- BCH_results_f %>% mutate(models_0.01_weights = log_regs_min_f)
#Because of the case weights we have to test the parameters manually via likelihood ratio tests
BCH_results_f <- BCH_results_f %>% mutate(
lr_1 = lapply(BCH_results_f$models, logistftest, test=1,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_2 = lapply(BCH_results_f$models, logistftest, test=2,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_3 = lapply(BCH_results_f$models, logistftest, test=3,
weights=BCH_expanded_dataframe_f$modal_weight_2),
lr_total = lapply(BCH_results_f$models, logistftest, weights=BCH_expanded_dataframe_f$modal_weight_2, control=logistf.control(maxit=10000))) #We get a warning: Maximum number of iterations for null model exceeded. Neither increasing the maximum number of iterations nor using a different start value (by changing the seed) help.
BCH_results_f <- BCH_results_f %>% mutate(
lr_1_0.01 = lapply(BCH_results_f$models, logistftest, test=1,
weights=BCH_expanded_dataframe_f$modal_weight_3,
control = logistf.control(fit="IRLS")),
lr_2_0.01 = lapply(BCH_results_f$models, logistftest, test=2,
weights=BCH_expanded_dataframe_f$modal_weight_3,
control = logistf.control(fit="IRLS")),
lr_3_0.01 = lapply(BCH_results_f$models, logistftest, test=3,
weights=BCH_expanded_dataframe_f$modal_weight_3,
control = logistf.control(fit="IRLS")),
lr_total_0.01 = lapply(BCH_results_f$models, logistftest,
weights=BCH_expanded_dataframe_f$modal_weight_3,
control = logistf.control(fit="IRLS")))
#We extract p-values from the likelihood ratio tests.
BCH_results_f <- BCH_results_f %>% mutate(
p_1=lapply(BCH_results_f$lr_1, function(x){x$prob}) %>% unlist(),
p_2=lapply(BCH_results_f$lr_2, function(x){x$prob}) %>% unlist(),
p_3=lapply(BCH_results_f$lr_3, function(x){x$prob}) %>% unlist(),
p_1_0.01=lapply(BCH_results_f$lr_1_0.01, function(x){x$prob}) %>% unlist(),
p_2_0.01=lapply(BCH_results_f$lr_2_0.01, function(x){x$prob}) %>% unlist(),
p_3_0.01=lapply(BCH_results_f$lr_3_0.01, function(x){x$prob}) %>% unlist(),
)
#Order BCH_results
BCH_results_f <- BCH_results_f[order(BCH_results_f$outcomes),]
#Add parameter estimates
#How to index the results_parameters table:
#All regression weights start with "b",
#All model output from the 0.01 weighted regression ends with "_0.01"
#All lower confidence interval limits begin with ci_l
#All upper confidence interval limits begin with ci_r
#All lr-test p-values start with p_lr
#Use tidyverse select functions
BCH_results_parameters_f <- tibble(outcomes = BCH_results_f$outcomes,
b_intercept = lapply(BCH_results_f$models, function(x){x$coefficients[1]}) %>% unlist(),
b_predicted_class_2 = lapply(BCH_results_f$models, function(x){x$coefficients[2]}) %>% unlist(),
b_predicted_class_3 =lapply(BCH_results_f$models, function(x){x$coefficients[3]}) %>% unlist(),
b_intercept_0.01 = lapply(BCH_results_f$models_0.01_weights, function(x){x$coefficients[1]}) %>%
unlist(),
b_predicted_class_2_0.01 = lapply(BCH_results_f$models_0.01_weights, function(x){x$coefficients[2]}) %>%
unlist(),
b_predicted_class_3_0.01 = lapply(BCH_results_f$models_0.01_weights, function(x){x$coefficients[3]}) %>%
unlist(),
ci_l_intercept = lapply(BCH_results_f$models, function(x){x$ci.lower[1]}) %>% unlist(),
ci_l_predicted_class_2 =lapply(BCH_results_f$models, function(x){x$ci.lower[2]}) %>% unlist(),
ci_l_predicted_class_3 =lapply(BCH_results_f$models, function(x){x$ci.lower[3]}) %>% unlist(),
ci_l_intercept_0.01= lapply(BCH_results_f$models_0.01_weights, function(x){x$ci.lower[1]}) %>% unlist(),
ci_l_predicted_class_2_0.01=lapply(BCH_results_f$models_0.01_weights, function(x){x$ci.lower[2]}) %>%
unlist(),
ci_l_predicted_class_3_0.01=lapply(BCH_results_f$models_0.01_weights, function(x){x$ci.lower[3]}) %>%
unlist(),
ci_r_intercept =lapply(BCH_results_f$models, function(x){x$ci.upper[1]}) %>% unlist(),
ci_r_predicted_class_2=lapply(BCH_results_f$models, function(x){x$ci.upper[2]}) %>% unlist(),
ci_r_predicted_class_3=lapply(BCH_results_f$models, function(x){x$ci.upper[3]}) %>% unlist(),
ci_r_intercept_0.01 = lapply(BCH_results_f$models_0.01_weights, function(x){x$ci.upper[1]}) %>% unlist(),
ci_r_predicted_class_2_0.01 = lapply(BCH_results_f$models_0.01_weights, function(x){x$ci.upper[2]}) %>%
unlist(),
ci_r_predicted_class_3_0.01  = lapply(BCH_results_f$models_0.01_weights, function(x){x$ci.upper[3]}) %>%
unlist(),
p_lr_intercept = lapply(BCH_results_f$lr_1, function(x){x$prob}) %>% unlist(),
p_lr_predicted_class_2 = lapply(BCH_results_f$lr_2, function(x){x$prob}) %>% unlist(),
p_lr_predicted_class_3 = lapply(BCH_results_f$lr_3, function(x){x$prob}) %>% unlist(),
p_lr_intercept_0.01 = lapply(BCH_results_f$lr_1_0.01, function(x){x$prob}) %>% unlist(),
p_lr_predicted_class_2_0.01 = lapply(BCH_results_f$lr_2_0.01, function(x){x$prob}) %>% unlist(),
p_lr_predicted_class_3_0.01 = lapply(BCH_results_f$lr_3_0.01, function(x){x$prob}) %>% unlist(),
p_lr_total = lapply(BCH_results_f$lr_total, function(x){x$prob}) %>% unlist(),
p_lr_total_0.01 = lapply(BCH_results_f$lr_total_0.01, function(x){x$prob}) %>% unlist())
# #Save output to RDS.
# BCH_results_f %>% saveRDS(file="./out/BCH_results_f.RDS")
#
# #Save Parameters Table to RDS
BCH_results_parameters_f %>% saveRDS(file="./out/BCH_results_parameters_f.RDS")
save.image("C:/Users/Rebecca/Documents/GitHub/LPA_analysis/.RData")
flextable(class_prob_lca_f) %>%
theme_booktabs()
#Do not show code chunks in the knitted document
knitr::opts_chunk$set(echo=F, warning=F, message=F)
#For reproducible results
set.seed(100)
#Turn off scientific notification
options(scipen=999)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM", "MOTE", "flextable", "logistf")
#Load function to check whether required packages are installed & load required packages
source("./functions/check_required_packages.R")
check_required_packages(packages)
lapply(packages, require, character.only=T)
#Clean up
rm(packages)
#######ATTENTION!!!
#At this point we load a pre-saved .RData file in which the environment is saved. That makes the code less reproducible, but removing the eval=F specifications in all code blocks will make that possible
load(".RData")
##DATA##
#We use gender instead of sex. A 1 represents a female. We want to only grab females.
data_females <- haven::read_spss("./data/data raw/dataE_2112.sav") %>% dplyr::select(gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% filter(gender == 1) %>% dplyr::select(-gender)
flextable(class_prob_lca_f) %>%
theme_booktabs()
flextable(as.data.frame(class_prob_lca_f)) %>%
theme_booktabs()
as.data.frame(class_prob_lca_f)
flextable(lca_fit_f) %>%
set_formatter(Entropy=function(x) format(round(x, 2), nsmall=2),
prob_min=function(x) format(round(x, 2), nsmall=2),
prob_max=function(x) format(round(x, 2), nsmall=2),
n_min=function(x) format(round(x, 2), nsmall=2),
np_ratio=function(x) format(round(x, 2), nsmall=2),
np_local=function(x) format(round(x, 2), nsmall=2)) %>%
set_header_labels(Name="Number of classes", LL="LogLikelihood", n="N", prob_min="Min prob", prob_max="Max prob", n_min="Min n", np_ratio="NP ratio", np_local="Local NP") %>%
theme_booktabs() %>%
autofit()
lca_fit_f
#Do not show code chunks in the knitted document
knitr::opts_chunk$set(echo=F, warning=F, message=F)
#For reproducible results
set.seed(100)
#Turn off scientific notification
options(scipen=999)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM", "MOTE", "flextable", "logistf")
#Load function to check whether required packages are installed & load required packages
source("./functions/check_required_packages.R")
check_required_packages(packages)
lapply(packages, require, character.only=T)
#Clean up
rm(packages)
#######ATTENTION!!!
#At this point we load a pre-saved .RData file in which the environment is saved. That makes the code less reproducible, but removing the eval=F specifications in all code blocks will make that possible
load(".RData")
##DATA##
#We use gender instead of sex. A 1 represents a female. We want to only grab females.
data_females <- haven::read_spss("./data/data raw/dataE_2112.sav") %>% dplyr::select(gender, CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr) %>% filter(gender == 1) %>% dplyr::select(-gender)
View(lca_fit_f)
lca_fit_f <- table_fit(lca_models_f) %>% select(Name, LL, n, Parameters, BIC, Entropy, prob_min, prob_max, n_min, np_ratio, np_local)
save.image("C:/Users/Rebecca/Documents/GitHub/LPA_analysis/.RData")
flextable(lca_fit_f) %>%
set_formatter(Entropy=function(x) format(round(x, 2), nsmall=2),
prob_min=function(x) format(round(x, 2), nsmall=2),
prob_max=function(x) format(round(x, 2), nsmall=2),
n_min=function(x) format(round(x, 2), nsmall=2),
np_ratio=function(x) format(round(x, 2), nsmall=2),
np_local=function(x) format(round(x, 2), nsmall=2)) %>%
set_header_labels(Name="Number of classes", LL="LogLikelihood", n="N", prob_min="Min prob", prob_max="Max prob", n_min="Min n", np_ratio="NP ratio", np_local="Local NP") %>%
theme_booktabs() %>%
autofit()
plot_prob(lca_final_model_f) +
scale_fill_grey(start = 0.9, end = 0.5, aesthetics = "fill") +
scale_x_discrete(labels=c("attr"="Attraction to children", "CSBD"="Compulsive sex.", "lon"="Loneliness", "meffort"="Mating effort", "mvalue"="Mate value", "PPCS_6"="Probl. porn use", "sexdrive2"="Sex drive", "socialanx"="Social anxiety")) +
labs(x="") +
theme_classic() +
theme(axis.text.x=element_text(angle=65, hjust=1, size=10))
