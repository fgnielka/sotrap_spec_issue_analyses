knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "psych")
#load function to check whether required packages are installed.
source("./functions/check_required_packages.R")
check_required_packages(packages)
#load required packages
lapply(packages, require, character.only=T)
data <- haven::read_spss("./data/data raw/dataE_clear5 2209.sav") %>% select(CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr)
data <- haven::read_spss("./data/data_raw/dataE_clear5 2209.sav") %>% select(CASE, sexdrive2, CSBD, PPCS_6, meffort, socialanx, lon, mvalue, attr)
View(data)
data <- haven::read_spss("./data/data_raw/dataE_clear5 2209.sav")
View(data)
colnames(data) %>% grep("y")
colnames(data) %>% grep("Y")
?starts_with()
data %>% select(starts_with("Y"))
data %>% select(starts_with("Y")) %>% colnames()
data %>% select(starts_with("Y")) %>% select(ends_with("r")) %>% head()
View(data)
data %>% select(CASE, starts_with("Y")) %>% select(CASE, ends_with("r")) %>% head()
data %>% select(CASE, starts_with("Y")) %>% select(CASE, ends_with("r")) %>% colnames()
data <- haven::read_spss("./data/data_raw/dataE_clear5 2209.sav") %>% select(CASE, starts_with("Y")) %>% select(CASE, ends_with("r"))
View(data)
?psych::fa()
tetrachoric(data %>% select(-CASE)) %>% psych::scree()
polychoric(data %>% select(-CASE)) %>% psych::scree()
polychoric(data %>% select(-CASE))
polychoric(data %>% select(-CASE)) %>% view()
polychoric(data %>% select(-CASE)) %>% as.matrix()
corr <- polychoric(data %>% select(-CASE)) %>% as.matrix()
View(corr)
corr <- polychoric(data %>% select(-CASE))
corr$rho
corr <- corr$rho
corr %>% class()
corr <- polychoric(data %>% select(-CASE))$rho
scree(polychoric(data %>% select(-CASE))$rho)
?psych::fa
efa_3 <- psych::fa(corr, nfactors=3, rotation="promax")
install.packages("GPArotation")
efa_3 <- psych::fa(corr, nfactors=3, rotation="promax")
summary(efa_3)
efa_3
print(efa_3, cut=.4, sort = T, digits=2)
efa_3 <- psych::fa(corr, nfactors=3, rotation="promax", fm="old.min")
efa_3 <- psych::fa(corr, nfactors=3, rotation="promax", fm="old.min", data=data %>% select(-CASE))
install.packages("jaspFactor")
efa_2 <- psych::fa(corr, nfactors=2, rotation="promax", fm="")
efa_2 <- psych::fa(corr, nfactors=2, rotation="promax")
load("C:/ARICA/WP3 RAs/Frederic/General_Assembly_2024/Relational_Analysis/.Rhistory")
save.image("C:/ARICA/WP3 RAs/Frederic/General_Assembly_2024/Relational_Analysis/.RData")
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "igraph")
#load required packages
lapply(packages, require, character.only=T)
#Define a function that takes the dataframe with topic_ids and vectors with member_ids of posters and returns a list of tibbles in which each unique possible combination of posters is reported in a combinations column.
get_unique_poster_tuples <- function(row, dataframe, column_of_poster_ids, column_of_topic_ids){
poster_vector <- dataframe %>% pull(column_of_poster_ids)
poster_vector <- poster_vector[[row]]
poster_dataframe <- expand_grid(names_1= poster_vector, names_2 = poster_vector) %>%
mutate(names_1 = as.numeric(names_1),
names_2 = as.numeric(names_2))
poster_dataframe <-  poster_dataframe %>%
mutate(combinations = lapply( 1:nrow(poster_dataframe),                                                                      function(x){c(poster_dataframe$names_1[x],poster_dataframe$names_2[x]) %>% sort()})) %>%
filter(names_1 != names_2) %>%
distinct(combinations, .keep_all = TRUE)
topic_id <- dataframe %>% pull(column_of_topic_ids)
topic_id <- rep(topic_id[[row]], times=nrow(poster_dataframe))
poster_dataframe <- poster_dataframe %>% mutate(topic_id = topic_id)
return(poster_dataframe)
}
#Enter the list of tibbles to our topic_network tibble.
posters_in_topic <- lapply(1:nrow(topic_network), get_unique_poster_tuples, dataframe=topic_network, column_of_poster_ids = "posters", column_of_topic_ids = "topic_id")
#We load the required data from the phpbb_total.sav file in the pedleaks folder.
cols <- c("post_id", "member_id", "topic_id", "forum_id", "post_original", "post_reply", "post_time")
data <- haven::read_spss("./data/phpbb_posts nested within members.sav") %>% select(all_of(cols))
"C:\ARICA\WP3 RAs\Frederic\General_Assembly_2024\Relational_Analysis\data\phpbb_posts nested within members.sav"
#We load the required data from the phpbb_total.sav file in the pedleaks folder.
cols <- c("post_id", "member_id", "topic_id", "forum_id", "post_original", "post_reply", "post_time")
data <- haven::read_spss("./data/phpbb_posts nested within members.sav") %>% select(all_of(cols))
topic_network <- data %>% as_tibble()%>% group_by(topic_id) %>% summarize(posters = stringr::str_c(member_id, collapse=",")) %>% mutate(posters = str_split(posters, pattern=","))
#Define a function that takes the dataframe with topic_ids and vectors with member_ids of posters and returns a list of tibbles in which each unique possible combination of posters is reported in a combinations column.
get_unique_poster_tuples <- function(row, dataframe, column_of_poster_ids, column_of_topic_ids){
poster_vector <- dataframe %>% pull(column_of_poster_ids)
poster_vector <- poster_vector[[row]]
poster_dataframe <- expand_grid(names_1= poster_vector, names_2 = poster_vector) %>%
mutate(names_1 = as.numeric(names_1),
names_2 = as.numeric(names_2))
poster_dataframe <-  poster_dataframe %>%
mutate(combinations = lapply( 1:nrow(poster_dataframe),                                                                      function(x){c(poster_dataframe$names_1[x],poster_dataframe$names_2[x]) %>% sort()})) %>%
filter(names_1 != names_2) %>%
distinct(combinations, .keep_all = TRUE)
topic_id <- dataframe %>% pull(column_of_topic_ids)
topic_id <- rep(topic_id[[row]], times=nrow(poster_dataframe))
poster_dataframe <- poster_dataframe %>% mutate(topic_id = topic_id)
return(poster_dataframe)
}
#Enter the list of tibbles to our topic_network tibble.
posters_in_topic <- lapply(1:nrow(topic_network), get_unique_poster_tuples, dataframe=topic_network, column_of_poster_ids = "posters", column_of_topic_ids = "topic_id")
topic_network <- topic_network %>% mutate(posters_in_topic = posters_in_topic)
#Enter a column with the number of posters in every topic
topic_network <- topic_network %>% mutate(n_posters = topic_network$posters %>% lapply(length))
rm(posters_in_topic)
#Filter out all values in which only one person has posted in the topic. In a network they would end up being dots without connections. We save them in a serperate tibble however, if one wishes to analyse them later on.
topic_network_unanswered_topics <- topic_network %>% filter(n_posters <  2)
topic_network_answered_topics <- topic_network %>% filter(n_posters > 1)
#We remove the topic_network tibble. We can always recreate it by combining the topic_network_answered_topics and topic_network_unanswered_topics tibbles.
rm(topic_network)
#We combine the tibbles of unique combinations of posters in all topics by column.
test <- map_dfr(topic_network_answered_topics$posters_in_topic, bind_rows)
#We now have a table that tells us who posts together in a topic. However we do not yet know who DOESN'T post together.
# To DO: Look up how an edge lists object has to look like in igraph. Find a way to add all posters who engage in threads with more than one post together and create a dummy variable with people who post together in one topic and people who do not post together in a topic. Maybe consider using group_by name_1 or something and sum to create a weighed edge list.
test_summary <- test %>% group_by(combinations) %>% reframe(combinations = combinations,topic_id = topic_id, n = n())
name_1 <- lapply(1:nrow(test_summary), function(x){test_summary$combinations[[x]][1]}) %>% flatten() %>% unlist()
name_2 <- lapply(1:nrow(test_summary), function(x){test_summary$combinations[[x]][2]}) %>% flatten() %>% unlist()
test_summary <- test_summary %>%  mutate(names_1 = name_1, names_2 = name_2)
test_summary <- test_summary %>% group_by(combinations) %>% reframe(topic_ids = paste(topic_id, collapse=","), n=n, names_1 = names_1, names_2 = names_2) %>% distinct()
test_summary <- test_summary %>% mutate(topic_ids = lapply(1:nrow(test_summary), function(x){test_summary$topic_ids[[x]] %>% str_split_1(",") %>% as.numeric()}))
#We combine the tibbles of unique combinations of posters in all topics by column.
topic_network_edges <- map_dfr(topic_network_answered_topics$posters_in_topic, bind_rows)
#We now have a table that tells us who posts together in a topic. However we do not yet know who DOESN'T post together.
# To DO: Look up how an edge lists object has to look like in igraph. Find a way to add all posters who engage in threads with more than one post together and create a dummy variable with people who post together in one topic and people who do not post together in a topic. Maybe consider using group_by name_1 or something and sum to create a weighed edge list.
topic_network_edges <- topic_network_edges %>% group_by(combinations) %>% reframe(combinations = combinations,topic_id = topic_id, n = n())
name_1 <- lapply(1:nrow(topic_network_edges), function(x){topic_network_edges$combinations[[x]][1]}) %>% flatten() %>% unlist()
name_2 <- lapply(1:nrow(topic_network_edges), function(x){topic_network_edges$combinations[[x]][2]}) %>% flatten() %>% unlist()
topic_network_edges <- topic_network_edges %>%  mutate(names_1 = name_1, names_2 = name_2)
topic_network_edges <- topic_network_edges %>% group_by(combinations) %>% reframe(topic_ids = paste(topic_id, collapse=","), n=n, names_1 = names_1, names_2 = names_2) %>% distinct()
topic_network_edges <- topic_network_edges %>% mutate(topic_ids = lapply(1:nrow(topic_network_edges), function(x){topic_network_edges$topic_ids[[x]] %>% str_split_1(",") %>% as.numeric()}))
#topic_network_edges is the output of our data wrangling. It includes a weighted undirected edgelist, that who posted in the same topics in pedoleaks (nodes = names_1, names_2; edge_weights = n). We know which topics these were, because a vector for each unique combination of users, who posted in the same topic, the ids of the common topics saved in the column "topic_ids".
rm(test, test_summary, efa_3, corr)
View(topic_network_edges)
topic_network <- list("answered_topics" = topic_network_answered_topics, "unanswered_topics" =topic_network_unanswered_topics, "edges_at" = topic_network_edges)
rm(topic_network_answered_topics, topic_network_edges, topic_network_unanswered_topics)
topic_network[["edges_at"]]
g <- graph_from_data_frame(topic_network[["edges_at"]] %>% rename(Source = names_1, Target = names_2, weight = n), directed=FALSE)
View(g)
print(g, e=TRUE, v=TRUE)
g <- graph_from_data_frame(topic_network[["edges_at"]] %>% select(names_1, names_1, n) %>% rename(Source = names_1, Target = names_2, weight = n), directed=FALSE)
g <- graph_from_data_frame(topic_network[["edges_at"]] %>% select(names_1, names_2, n) %>% rename(Source = names_1, Target = names_2, weight = n), directed=FALSE)
print(g, e=TRUE, v=TRUE)
attr(g, "weight")
attributes(g)
E(g)
topic_network[["edges_at"]] %>% pull(n)
E(g)$weight <- topic_network[["edges_at"]] %>% pull(n)
print(g, e=TRUE, v=TRUE)
is_weighted(g)
vertex_connectivity(g)
degree(g, v=V(g))
degree(g, v=V(g)) %>% view()
topic_graph <- graph_from_data_frame(topic_network[["edges_at"]] %>% select(names_1, names_2, n) %>% rename(Source = names_1, Target = names_2, weight = n), directed=FALSE)
E(topic_graph)$weight <- topic_network[["edges_at"]] %>% pull(n)
print(g, e=TRUE, v=TRUE)
View(topic_network)
centrality_topic_graph <- degree(topic_graph, v=V(topic_graph)) %>% as.tibble() %>% rownames_to_column("user_id")
centrality_topic_graph <- degree(topic_graph, v=V(topic_graph)) %>% as_tibble() %>% rownames_to_column("user_id")
View(centrality_topic_graph)
centrality_topic_graph <- degree(topic_graph, v=V(topic_graph)) %>% as.tibble() %>% rename(degree = value) %>% rownames_to_column("user_id")
centrality_topic_graph <- degree(topic_graph, v=V(topic_graph))  %>% rename(degree = value) %>% rownames_to_column("user_id")
centrality_topic_graph <- degree(topic_graph, v=V(topic_graph))  %>% rownames_to_column("user_id")
centrality_topic_graph <- degree(topic_graph, v=V(topic_graph))
centrality_topic_graph %>% class()
rownames(centrality_topic_graph)
centrality_topic_graph <- degree(topic_graph, v=V(topic_graph))
centrality_topic_graph %>% rownames()
centrality_topic_graph %>% attributes()
centrality_topic_graph %>% names()
degree(topic_graph, v=V(topic_graph))
centrality_topic_graph <- tibble("user_id" = degree(topic_graph, v=V(topic_graph))$names, "degree" = degree(topic_graph, v=V(topic_graph)))
centrality_topic_graph <- tibble("user_id" = degree(topic_graph, v=V(topic_graph)) %>% names(), "degree" = degree(topic_graph, v=V(topic_graph)))
View(centrality_topic_graph)
degree(topic_graph, v=V(topic_graph), normalized=T)
betweenness(topic_graph)
strength(topic_graph)
centrality_topic_graph <- tibble("user_id" = degree(topic_graph, v=V(topic_graph)) %>% names(), "degree" = degree(topic_graph, v=V(topic_graph))) %>% mutate(degree_centralized = degree(topic_graph, v=V(topic_graph), centralized=T), weigthed_degree = strength(topic_graph), weighted_degree_centralized = strength(topic_graph, centralized=T))
centrality_topic_graph <- tibble("user_id" = degree(topic_graph, v=V(topic_graph)) %>% names(), "degree" = degree(topic_graph, v=V(topic_graph))) %>% mutate(degree_normalized = degree(topic_graph, v=V(topic_graph), normalized = T), weigthed_degree = strength(topic_graph), weighted_degree_normalized = strength(topic_graph, normalized=T))
centrality_topic_graph <- tibble("user_id" = degree(topic_graph, v=V(topic_graph)) %>% names(), "degree" = degree(topic_graph, v=V(topic_graph))) %>% mutate(degree_normalized = degree(topic_graph, v=V(topic_graph), normalized = T), weigthed_degree = strength(topic_graph))
vertex_connectivity(topic_graph)
View(centrality_topic_graph)
load("C:/Users/gniel/OneDrive - MSB Medical School Berlin/Project special issue/LPA_analysis/LCA_analysis.Rmd")
load("C:/Users/gniel/OneDrive - MSB Medical School Berlin/Project special issue/LPA_analysis/.RData")
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM")
#load function to check whether required packages are installed.
source("./functions/check_required_packages.R")
check_required_packages(packages)
#load required packages
lapply(packages, require, character.only=T)
#######ATTENTION:
#At this point I load a presaved RData variables in which the  environment is saved. That makes the code less reproduceable, but removing the eval=FALSE specifications in all code blocks will make that possible.
load(".RData")
options(scipen = 999)
View(prob_table_LCA_m)
prob_table_LCA_m <- reshape(prob_table_LCA_m, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))
View(prob_table_LCA_m)
save.image("C:/Users/gniel/OneDrive - MSB Medical School Berlin/Project special issue/LPA_analysis/.RData")
prob_table_LCA_f <- reshape(prob_table_LCA_f, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category")) %>% view()
prob_table_LCA_f <- reshape(prob_table_LCA_f, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))
prob_table_LCA_f <- reshape(prob_table_LCA_f, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))
prob_table_LCA_f <- table_prob(lca_final_model_f)
prob_table_LCA_f <- reshape(prob_table_LCA_f, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))
save.image("C:/Users/gniel/OneDrive - MSB Medical School Berlin/Project special issue/LPA_analysis/.RData")
View(data)
save.image("C:/Users/gniel/OneDrive - MSB Medical School Berlin/Project special issue/LPA_analysis/.RData")
install.packages("foreign")
prob(lca_final_model_m, type="individual")
library(tidyverse)
library(tidySEM)
prob(lca_final_model_m, type="individual")
class_prob(lca_final_model_m, type="individual")
posterior_probabilities <-  class_prob(lca_final_model_m, type="individual") %>% as_tibble()
posterior_probabilities
posterior_probabilities %>% colnames()
posteriors <-  class_prob(lca_final_model_m, type="individual") %>% as_tibble()
rm(posterior_probabilities)
length(posteriors)
nrow(posteriors)
ncol(posteriors)
posterios
posteriors
posteriorsstr()
posteriors %>% str()
class_prob(lca_final_model_m, type="individual") %>% str()
class_prob(lca_final_model_m, type="individual") %>% unlist()
class_prob(lca_final_model_m, type="individual") %>% flatten() %>% str()
class_prob(lca_final_model_m, type="individual") %>% str()
class_prob(lca_final_model_m, type="individual")$individual %>% str()
class_prob(lca_final_model_m, type="individual")$individual %>% as_tibble() %>% str()
posteriors <- class_prob(lca_final_model_m, type="individual")$individual %>% as_tibble()
posteriors %>% str()
posteriors %>% colnames()
posteriors %>% ncol()
ncol(posteriors)^2
ncol(posteriors - 1)^2
(ncol(posteriors)-1)^2
posteriors$predicted
#creating the weights needed for the D matrix calculation
n<-length(nrow(posteriors)) # the length of the data file
n_class <- ncol(posteriors)-1) #number of classes in the lca solution
ctm<-matrix(nrow=n,ncol=n_class^2)#modal weights 3*3 class
ctp<-matrix(nrow=n,ncol=n_class^2)#proportional weights 3*3 class
modal<-matrix(nrow=n, ncol=n_class^2) #modal class assignment 3 dummies for 3 classes
for (j in 1:n_class) # creating dummies for modal posterior class assignment
{
modal[,j]<- ifelse (posteriors$predicted==j, 1, 0)
}
n_class <- ncol(posteriors-1) #number of classes in the lca solution
ctm<-matrix(nrow=n,ncol=n_class^2)#modal weights 3*3 class
ctp<-matrix(nrow=n,ncol=n_class^2)#proportional weights 3*3 class
modal<-matrix(nrow=n, ncol=n_class^2) #modal class assignment 3 dummies for 3 classes
for (j in 1:n_class) # creating dummies for modal posterior class assignment
{
modal[,j]<- ifelse (posteriors$predicted==j, 1, 0)
}
n
nrow(posteriors)
#creating the weights needed for the D matrix calculation
n<-nrow(posteriors) # the length of the data file
n_class <- ncol(posteriors-1) #number of classes in the lca solution
ctm<-matrix(nrow=n,ncol=n_class^2)#modal weights 3*3 class
ctp<-matrix(nrow=n,ncol=n_class^2)#proportional weights 3*3 class
modal<-matrix(nrow=n, ncol=n_class^2) #modal class assignment 3 dummies for 3 classes
for (j in 1:n_class) # creating dummies for modal posterior class assignment
{
modal[,j]<- ifelse (posteriors$predicted==j, 1, 0)
}
view(modal)
#creating the weights needed for the D matrix calculation
n<-nrow(posteriors) # the length of the data file
n_class <- ncol(posteriors-1) #number of classes in the lca solution
ctm<-matrix(nrow=n,ncol=n_class)#modal weights 3*3 class
ctp<-matrix(nrow=n,ncol=n_class)#proportional weights 3*3 class
modal<-matrix(nrow=n, ncol=n_class) #modal class assignment 3 dummies for 3 classes
for (j in 1:n_class) # creating dummies for modal posterior class assignment
{
modal[,j]<- ifelse (posteriors$predicted==j, 1, 0)
}
view(modal)
n_class
#creating the weights needed for the D matrix calculation
n<-nrow(posteriors) # the length of the data file
n_class <- ncol(posteriors)-1 #number of classes in the lca solution
ctm<-matrix(nrow=n,ncol=n_class)#modal weights 3*3 class
ctp<-matrix(nrow=n,ncol=n_class)#proportional weights 3*3 class
modal<-matrix(nrow=n, ncol=n_class) #modal class assignment 3 dummies for 3 classes
for (j in 1:n_class) # creating dummies for modal posterior class assignment
{
modal[,j]<- ifelse (posteriors$predicted==j, 1, 0)
}
modal
view(ctm)
??elementwise
posteriors %>% select(-1)
posteriors %>% select(-predicted) %>% as_matrix
posteriors %>% select(-predicted) %>% as_matrix()
posteriors %>% select(-predicted) %>% as.matrix()
(posteriors %>% select(-predicted) %>% as.matrix())*modal
modla
modal
?solve()
modal
# obtaining the elements of the D matrix
ctm <- (posteriors %>% select(-predicted) %>% as.matrix())*modal
combined <- c(modal, ctm)
combined
combined <- cbind(modal, ctm)
combined
combined %>% str(+)
combined %>% str()
combined %>% class()
colSums(cobined)
colSums(combined)
class_prob(lca_final_model_m, type="avg.mostlikely")
modal %>% head()
combined %>% head()
apply(combined,2,sum)
apply(combined,2,sum) %>% cbind() %>% class()
apply(combined,2,sum) %>% cbind() %>% str()
apply(combined,2,sum) %>% str()
colSums()
COLSUMS
COLSUMS<- apply(combined,2,sum) %>% cbind() #summing all the weights
COLSUMS
4:12 %>% length()
ctm
apply(posteriors %>% select(-predicted),1,which.max)
posteriors <- class_prob(lca_final_model_m, type="individual")$individual %>% select(-predicted)
modclass <- apply(posteriors,1,which.max)
nclass=3
Ptable <- cbind(posteriors, modclass)
Pmatrix <- matrix(0, nclass, nclass)
Npmatrix <- matrix(0, nclass, nclass)
for (i in 1:nclass){
for (j in 1:nclass){
Pmatrix[i,j]<-sum(subset(Ptable, modclass==i)[,j])
Npmatrix[i,j] <- Pmatrix[i,j]*table(modclass)[i]
}
}
view(Npmatrix)
view(Pmatrix)
table(modclass)
modclass
posteriors <- class_prob(lca_final_model_m, type="individual")$individual %>% select(-predicted)
posteriors <- class_prob(lca_final_model_m, type="individual")$individual %>% select(-predicted)
posteriors <- class_prob(lca_final_model_m, type="individual")$individual %>% as_tibble() %>% select(-predicted)
modclass <- apply(posteriors,1,which.max)
nclass=3
Ptable <- cbind(posteriors, modclass)
Pmatrix <- matrix(0, nclass, nclass)
Npmatrix <- matrix(0, nclass, nclass)
for (i in 1:nclass){
for (j in 1:nclass){
Pmatrix[i,j]<-sum(subset(Ptable, modclass==i)[,j])
Npmatrix[i,j] <- Pmatrix[i,j]*table(modclass)[i]
}
}
view(Npmatrix)
view(Pmatrix)
denom<-colSums(Npmatrix)
Qmatrix<-matrix(0, nclass, nclass)
for (i in 1:nclass){
for (j in 1:nclass){
Qmatrix[j,i]<-Npmatrix[i,j]/denom[j]
}
}
view(Qmatrix)
sum(subset(Ptable,modclass==1)
)
sum(subset(Ptable,modclass==1)[,1])
subset(Ptable,modclass==1)[,1]
subset(Ptable,modclass==1)
table(modclass)[1]
iD <- solve(Qmatrix)
View(iD)
combined
DIM <- solve(Qmatrix)
#final modal bch weights applied to each case i
wm1<- ((combined[,1]*DIM[1,1]) + ( combined[,2]
*DIM[2,1]) + (combined[,3]*DIM[3,1]))
wm2<- ((combined[,1]*DIM[1,2]) + (combined[,2]
*DIM[2,2]) + (combined[,3]*DIM[3,2]))
wm3<- ((combined[,1]*DIM[1,3]) + ( combined[,2]
*DIM[2,3]) + (combined[,3]*DIM[3,3]))
wm1 %>% length()
#create and save long file
class_longa<-data.frame (wmodal1=combined[,1],
wmodal2=combined[,2],
wmodal3=combined[,3],
wbchmodal1=wm1, wbchmodal2=wm2,
bchmodal3=wm3)
install.packages("Hmisc")
library(Hmisc)
View(class_longa)
#create and save long file
class_longa<-data.frame (wmodal1=combined[,1],
wmodal2=combined[,2],
wmodal3=combined[,3],
wbchmodal1=wm1, wbchmodal2=wm2,
wbchmodal3=wm3)
class_long<- reShape(class_longa, base=c("wmodal","wbchmodal"),reps=3)
View(class_long)
View(class_longa)
View(combined)
wm1
wm1[1]
(combined[,1]*DIM[1,1]) + ( combined[,2]
*DIM[2,1]) + (combined[,3]*DIM[3,1])
DIM %>% head()
DIM[1,1]
knitr::opts_chunk$set(echo = TRUE)
#for reproducible results
set.seed(100)
#Declare vector of required packages
packages <- c("tidyverse", "readr", "tidySEM", "rio", "ggpubr", "logistf")
#load function to check whether required packages are installed.
source("./functions/check_required_packages.R")
check_required_packages(packages)
#load required packages
lapply(packages, require, character.only=T)
#######ATTENTION:
#At this point I load a pre-saved .RData file in which the environment is saved. That makes the code less reproducible, but removing the eval=FALSE specifications in all code blocks will make that possible.
load(".RData")
#turn off scientific notification
options(scipen = 999)
lca_models_m_2.2 <- mx_lca(data=data_males %>% select(-CASE),5:7, run=TRUE, exhaustive=FALSE, checkHess=TRUE)
setwd("C:/Users/gniel/OneDrive - MSB Medical School Berlin/Project special issue/LPA_analysis")
saveRDS(lca_models_m_2.2, "./out/lca_models_m_2.2.RDS")
lca_models_m_1 <- read_rds("./out/lca_models_m_2.1.RDS")
lca_models_m_2 <- read_rds(".out/lca_models_m_2.2.RDS")
lca_models_m_2 <- read_rds("./out/lca_models_m_2.2.RDS")
test <- c(lca_models_m_1, lca_models_m_2) %>% flatten()
test <- c(lca_models_m_1, lca_models_m_2)
View(test)
View(lca_models_m)
table_fit(test)
class_prob(lca_models_m[[3]])
class_prob(lca_models_m[[3]], "sum.posterior")
lca_models_m <- c(lca_models_m_1, lca_models_m_2)
lca_fit <- table_fit(lca_models_m) %>% select(Name, LL, n, Parameters, BIC, Entropy, prob_min, prob_max, n_min, np_ratio, np_local)
LR_lca_m <- lr_lmr(lca_models_m)
lca_final_model_m <- lca_models_m[[3]] #x is the class enumeration of choice
#Check results
table_LCA_m <- table_results(lca_final_model_m)
table_LCA_m
lca_final_model_m %>% summary()
#conditional item probabilities
prob_table_LCA_m <- table_prob(lca_final_model_m)
prob_table_LCA_m <- reshape(prob_table_LCA_m, direction="wide", v.names="Probability", timevar="group", idvar = c("Variable", "Category"))
#class proportions
class_prob_lca_m <- class_prob(lca_final_model_m, "sum.posterior")
plot(lca_fit)
#class proportions
class_prob_lca_m
class_prob(lca_final_model_m, type="avg.mostlikely")
#Load precoded functions
source("./functions/D_matrix_modal.R")
source("./functions/modal_weights.R")
#First extract posterior probabilities for class assignment and most likely class membership
posteriors <- class_prob(lca_final_model_m, type="individual")$individual %>% as_tibble()
#Compute the inverse D matrix as in Bakk et al. (2013)
inverse_D <- solve(D_matrix_modal(posteriors))
#Extract a vector with the most likely class memberships
modal_assignments <- posteriors %>% select(predicted)
#Use the modal_weights function to create modal weights from the inverse_D matrix and combine the results in a tibble with the posterior probabilities of class membership and the most likely class membership.
BCH_expanded_dataframe <- tibble(posteriors, modal_weights(modal_assignments, inverse_D) %>% as_tibble())
colnames(BCH_expanded_dataframe) <- c("postprob_class_1", "postprob_class_2", "postprob_class_3", "modal_class", "modal_weight_1", "modal_weight_2", "modal_weight_3")
#Tidy up behind us
rm(modal_assignments, inverse_D, posteriors)
#Attach the CASE variable to our weighted dataframe.
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(CASE = data_males$CASE)
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% pivot_longer(cols=starts_with("modal_weight"), values_to = "modal_weight", names_to=NULL)
#Create a variable that encodes the assumed class assignments for the BCH analysis
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(predicted_class = vctrs::vec_rep(1:3, nrow(data_males)))
#Create dummy variables from this variable
BCH_expanded_dataframe <- fastDummies::dummy_cols(BCH_expanded_dataframe, select_columns = "predicted_class")
#Combine the expanded dataframe with the outcome variables
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% full_join(y=haven::read_spss("./data/data raw/dataE_clear5 2209.sav") %>% filter(gender == 2) %>% select(CASE, starts_with("Y_") & ends_with("r")) %>% mutate(CASE = CASE %>% as.numeric()),by="CASE")
#Set up a dataframe for the the outputs of the regression analysis. Each row corresponds to one item.
BCH_results <- tibble(outcomes = colnames(BCH_expanded_dataframe %>% select(starts_with("Y_"))))
#Fixing negative weights to zero or nigh-zero
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(modal_weight_2 = case_when(BCH_expanded_dataframe$modal_weight < 0 ~ 0, .default = BCH_expanded_dataframe$modal_weight))
BCH_expanded_dataframe <- BCH_expanded_dataframe %>% mutate(modal_weight_3 = case_when(BCH_expanded_dataframe$modal_weight < 0 ~ 0.01, .default = BCH_expanded_dataframe$modal_weight))
#We prepare the model specifications for the Firth Regression to run.
outcomes <- BCH_expanded_dataframe %>% select(starts_with("Y_")) %>% colnames()
predictors <- BCH_expanded_dataframe %>% select(starts_with("predicted")) %>% select(-predicted_class, -predicted_class_1) %>% colnames() %>% paste(collapse=" + ")
models <- paste(outcomes, "~", predictors, sep=" ")
#Do Flic (Firths regression with intercept correction)for the models in the list with weights fixed to zero.
log_regs <- list()
for (i in 1:17){
log_regs[[i]] <- logistf::logistf(formula=models[i], data = BCH_expanded_dataframe, model=TRUE, weights=BCH_expanded_dataframe$modal_weight_2) %>% flic()
}
##Do Flic for the models in the list with weights fixed to 0.01
log_regs_min <- list()
for (i in 1:17){
log_regs_min[[i]] <- logistf::logistf(formula=models[i], data = BCH_expanded_dataframe, model=TRUE, weights=BCH_expanded_dataframe$modal_weight_3) %>% flic()
}
#Combine the results in a dataframe
BCH_results <- BCH_results %>% mutate(models = log_regs)
BCH_results <- BCH_results %>% mutate(models_0.01_weights = log_regs_min)
View(BCH_results)
lca_models_f %>% class()
lca_models_m %>% class()
test <- as(lca_models_m, "mixture_list")
lr_lmr(lca_final_model_m)
lr_lmr(lca_final_model_m, lca_models_m[[2]])
save.image("C:/Users/gniel/OneDrive - MSB Medical School Berlin/Project special issue/LPA_analysis/.RData")
